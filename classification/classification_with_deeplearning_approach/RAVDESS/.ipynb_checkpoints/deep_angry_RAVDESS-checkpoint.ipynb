{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5591,
     "status": "ok",
     "timestamp": 1596102158762,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "B9TmOS9AFg61",
    "outputId": "45e0f246-230b-4aa2-c060-509a7686b0d7"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Numpy, pandas ans os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# matplotlib for displaying the output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librosa for audio\n",
    "import librosa\n",
    "\n",
    "\n",
    "#parselmouth for audio\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "\n",
    "#essentia\n",
    "\n",
    "import essentia.standard\n",
    "import essentia.streaming\n",
    "from essentia.standard import *\n",
    "\n",
    "\n",
    "#librairies for classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold, cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, make_scorer, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "#Deep learning\n",
    "\n",
    "### Plot imports ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Time Distributed ConvNet imports ###\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, TimeDistributed, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, LeakyReLU, Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import seaborn as sns \n",
    "from keras.utils import np_utils\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#for warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category= ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category= UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category= RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKH47UdIodVo"
   },
   "source": [
    "Dataframe to match audio with emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9028,
     "status": "ok",
     "timestamp": 1596102204943,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "6IAO4Lt4pfBi",
    "outputId": "c6bc92e1-7f21-438a-eb16-70d359ce50ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>surprised/03-01-08-02-01-02-17_norm_outNoise.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry/03-01-05-01-02-01-14_norm_outNoise.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy/03-01-03-01-01-02-16_norm_outNoise.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral/03-01-01-01-02-01-14_norm_outNoise.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry/03-01-05-02-01-01-08_norm_outNoise.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              audio label\n",
       "0  surprised/03-01-08-02-01-02-17_norm_outNoise.wav     0\n",
       "1      angry/03-01-05-01-02-01-14_norm_outNoise.wav     1\n",
       "2      happy/03-01-03-01-01-02-16_norm_outNoise.wav     0\n",
       "3    neutral/03-01-01-01-02-01-14_norm_outNoise.wav     0\n",
       "4      angry/03-01-05-02-01-01-08_norm_outNoise.wav     1"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parent_dir = \"ravdess\" #name of the folder containing the emotions\n",
    "\n",
    "def prepare_datadf(parent_dir): # a function whose parameter is the audio folder\n",
    "    df = pd.DataFrame(columns = ['audio', 'label']) #dataframe columns\n",
    "    \n",
    "    for  fichier_audio in os.listdir(parent_dir): # for each element in the audio folder\n",
    "        folder_path = os.path.join(parent_dir, fichier_audio) # path of each item  in the audio folder\n",
    "        \n",
    "       \n",
    "        \n",
    "        if(os.path.isdir(folder_path)): \n",
    "            audios = os.listdir(folder_path) #content of each emotional file\n",
    "            for i in audios:\n",
    "                emotion = None\n",
    "                if i.endswith('outNoise.wav'):\n",
    "                    if i[7] == '5':     ##this specifies that we class angry emotion against the others\n",
    "                                    #5 represents the 7th column of the file name\n",
    "                                    # This number varies for each emotion(ex calm = '2', fearfull = '7')\n",
    "                        emotion = 1\n",
    "                    \n",
    "                    else:\n",
    "                        emotion = 0\n",
    "                    df = df.append(pd.DataFrame({'audio':[os.path.join(fichier_audio, i)], 'label':[emotion]}), \n",
    "                           ignore_index=True) #adding values to the defined df:\n",
    "                                            #the audio column will take the audios_path, \n",
    "                                            #and the emotion column will take the corresponding emotion, ie the name of the folder\n",
    "    #Shuffling for randomness\n",
    "    df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "    return df\n",
    "datadf = prepare_datadf(parent_dir) #function call\n",
    "display(datadf.head()) #dataframe display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dr4_HGmdH_hY"
   },
   "source": [
    "Number of labels 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1596102222181,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "3_Rz5am4IBEV",
    "outputId": "aef7327a-403e-4256-e831-b60e835ebbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1053\n",
      "1     192\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "array=datadf.values\n",
    "audios=array[:,0]\n",
    "emotions=array[:,1]\n",
    "print(datadf.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DM9Dsr6nGdQK"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWiD09QxGpVJ"
   },
   "source": [
    "Function for framing and windowing the audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhgtSddTGvNT"
   },
   "outputs": [],
   "source": [
    "def fram_window(audio_path):\n",
    "    loader = essentia.standard.MonoLoader(filename= audio_path)\n",
    "\n",
    "    # and then we actually perform the loading:\n",
    "    audio = loader()\n",
    "\n",
    "    w = Windowing(type = 'hann')\n",
    "    spectrum = Spectrum() \n",
    "    #default parameter (hopsize and framesize)\n",
    "    hopSize = 512\n",
    "    frameSize = 1024 \n",
    "    for frame in FrameGenerator(audio, frameSize=1024, hopSize=512, startFromZero=True):\n",
    "        spect = spectrum(w(frame))\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5G6NwKlG8JW"
   },
   "source": [
    "function for features extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path):\n",
    "    features = []\n",
    "    \n",
    "    \n",
    "    #Load audios with the different libraries\n",
    "      \n",
    "    y,sr = librosa.load(audio_path)\n",
    "    sound = parselmouth.Sound(audio_path)\n",
    "    spec =  fram_window(audio_path) \n",
    "    \n",
    "    \n",
    "    #prosodics features\n",
    "    \n",
    "    pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    \n",
    "    duration = librosa.get_duration(y= spec, sr=sr)\n",
    "    energy = np.sum(spec ** 2) / np.float64(len(spec))\n",
    "    \n",
    "    \n",
    "    #spectrales features\n",
    "            \n",
    "    lpc = librosa.core.lpc(spec,16)         \n",
    "    mfcc = librosa.feature.mfcc(y= spec, sr=sr, n_mfcc = 13)\n",
    "    \n",
    "                \n",
    "    pointProcess = call(sound, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    \n",
    "        \n",
    "    formants = call(sound, \"To Formant (burg)\", 0.0, 5, 5500, 0.025, 100)\n",
    "    numPoints = call(pointProcess, \"Get number of points\")\n",
    "\n",
    "    f1_list = []\n",
    "    f2_list = []\n",
    "    f3_list = []\n",
    "    f4_list = []\n",
    "    \n",
    "    # Measure formants only at glottal pulses\n",
    "    for point in range(0, numPoints):\n",
    "        point += 1\n",
    "        t = call(pointProcess, \"Get time from index\", point)\n",
    "        f1 = call(formants, \"Get value at time\", 1, t, 'Hertz', 'Linear')\n",
    "        f2 = call(formants, \"Get value at time\", 2, t, 'Hertz', 'Linear')\n",
    "        f3 = call(formants, \"Get value at time\", 3, t, 'Hertz', 'Linear')\n",
    "        f4 = call(formants, \"Get value at time\", 4, t, 'Hertz', 'Linear')\n",
    "        f1_list.append(f1)\n",
    "        f2_list.append(f2)\n",
    "        f3_list.append(f3)\n",
    "        f4_list.append(f4)\n",
    "        \n",
    "    f1_list = [f1 for f1 in f1_list if str(f1) != 'nan']\n",
    "    f2_list = [f2 for f2 in f2_list if str(f2) != 'nan']\n",
    "    f3_list = [f3 for f3 in f3_list if str(f3) != 'nan']\n",
    "    \n",
    "    f4_list = [f4 for f4 in f4_list if str(f4) != 'nan']\n",
    "\n",
    "    f1_mean = statistics.mean(f1_list)\n",
    "    f2_mean = statistics.mean(f2_list)\n",
    "    f3_mean = statistics.mean(f3_list)\n",
    "    f4_mean = statistics.mean(f4_list)\n",
    "    \n",
    "    #voice activity features\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(spec)\n",
    "    \n",
    "    #voice quality features\n",
    "        \n",
    "    harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    HNR = call(harmonicity, \"Get mean\", 0, 0)\n",
    "    localJitter = call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    localabsoluteJitter = call(pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "\n",
    "    localShimmer =  call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    localdbShimmer = call([sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    \n",
    "    rapJitter = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    ppq5Jitter = call(pointProcess, \"Get jitter (ppq5)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    ddpJitter = call(pointProcess, \"Get jitter (ddp)\", 0, 0, 0.0001, 0.02, 1.3)   \n",
    "            \n",
    "    apq3Shimmer = call([sound, pointProcess], \"Get shimmer (apq3)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    aqpq5Shimmer = call([sound, pointProcess], \"Get shimmer (apq5)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    apq11Shimmer =  call([sound, pointProcess], \"Get shimmer (apq11)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    ddaShimmer = call([sound, pointProcess], \"Get shimmer (dda)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    \n",
    "    features.append(mean_pitch)\n",
    "    features.append(duration)\n",
    "    features.append(energy)\n",
    "    features.append(np.mean(zcr))\n",
    "    features.append(np.mean(lpc))\n",
    "    \n",
    "        \n",
    "    features.append(np.mean(mfcc))\n",
    "    \n",
    "    \n",
    "    features.append(HNR)\n",
    "    \n",
    "    features.append(localJitter)\n",
    "    features.append(np.mean(localabsoluteJitter))\n",
    "    \n",
    "    features.append(localShimmer)\n",
    "    features.append(localdbShimmer)\n",
    "    features.append(f1_mean)   \n",
    "    features.append(f2_mean)\n",
    "    features.append(f3_mean)\n",
    "    features.append(f4_mean)\n",
    "        \n",
    "    features.append(rapJitter)\n",
    "    features.append(ppq5Jitter)\n",
    "    features.append(ddpJitter)\n",
    "    \n",
    "    features.append(apq3Shimmer)\n",
    "    features.append(aqpq5Shimmer)\n",
    "    features.append(apq11Shimmer)\n",
    "    features.append(ddaShimmer)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqLDut92HWAf"
   },
   "source": [
    "Application of features extraction function on all audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4HYtF5eHXRr"
   },
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for audio_file in array[:,0]:\n",
    "    if audio_file.endswith('.wav'):\n",
    "        \n",
    "        features = extract_features(parent_dir+'/'+audio_file)\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1596105014339,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "x8PZZgEyUeYX",
    "outputId": "52bfe706-cd5e-4803-d645-336cedd05374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245\n"
     ]
    }
   ],
   "source": [
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNvIDRVAUpD3"
   },
   "source": [
    "Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDxfO5SJUss2"
   },
   "outputs": [],
   "source": [
    "encod = preprocessing.LabelEncoder()\n",
    "emotions = array[:,1]\n",
    "encod.fit(emotions)\n",
    "list(encod.classes_)\n",
    "labels=encod.transform(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atpDw444U3tg"
   },
   "source": [
    "Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAI6k0k1U5I6"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(all_features)\n",
    "X_scaler = scaler.transform(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hENmg0CTVBrQ"
   },
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1596105080250,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "OpQA2jnHVC3M",
    "outputId": "c161bcbc-5dc3-4be6-ed81-75ad86b6da4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 656\n",
      "After OverSampling, counts of label '0': 1053\n"
     ]
    }
   ],
   "source": [
    "ada = ADASYN(sampling_strategy = 0.6)\n",
    "X, y = ada.fit_sample(X_scaler, labels.ravel())\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dy5_XTIhVSpm"
   },
   "source": [
    "Process to select features after oversampling with ADASYN : the code first takes in a list the position of the features that are deleted, during the 1000 iterations, then uses a dataframe to count them. we notice that the features \"[1, 2, 3, 4, 9, 10, 13, 20] \" are deleted 727 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25107,
     "status": "ok",
     "timestamp": 1596105123602,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "NtMPEzopVUKN",
    "outputId": "f34ba5e6-9822-4bcc-df19-579eb0a43b73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>X_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 9, 10, 13, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 9, 10, 13, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 3, 4, 9, 10, 13, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 4, 9, 10, 13, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 4, 9, 10, 13, 20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iteration                    X_removed\n",
       "0         1  [1, 2, 3, 4, 9, 10, 13, 20]\n",
       "1         2  [1, 2, 3, 4, 9, 10, 13, 20]\n",
       "2         3  [1, 2, 3, 4, 9, 10, 13, 20]\n",
       "3         4  [1, 2, 3, 4, 9, 10, 13, 20]\n",
       "4         5  [1, 2, 3, 4, 9, 10, 13, 20]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurrences of features that are removed :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 9, 10, 13, 20]        727\n",
       "[1, 2, 3, 4, 9, 10, 13, 19, 20]    198\n",
       "[1, 2, 3, 4, 9, 10, 20]             57\n",
       "[1, 2, 3, 4, 9, 10, 19, 20]         18\n",
       "Name: X_removed, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compt=0\n",
    "df = pd.DataFrame(columns = ['iteration', 'X_removed'])\n",
    "while compt < 1000:\n",
    "    ada = ADASYN(sampling_strategy = 0.6)\n",
    "    \n",
    "    X, y = ada.fit_sample(X_scaler, labels.ravel())\n",
    "    X = np.asarray(X)\n",
    "    Kbest = SelectKBest(k=\"all\")\n",
    "    selec_features = Kbest.fit(X, y)\n",
    "    alpha = 0.01\n",
    "    #remove non_signifiant features selection\n",
    "    X_selec = X[:,np.where(selec_features.pvalues_ < alpha)[0]]\n",
    "    \n",
    "    pos_removed = []    \n",
    "    for i in range(len(X[0])):\n",
    "   \n",
    "        if X[0][i] not in X_selec[0]:\n",
    "            #print(i)\n",
    "            pos_removed.append(i)\n",
    "            str_pos_removed = str(pos_removed)\n",
    "    #print(pos_removed)\n",
    "    \n",
    "    compt = compt + 1\n",
    "    df= df.append(pd.DataFrame({'iteration':[compt], 'X_removed':[str_pos_removed]}), ignore_index=True)\n",
    "display(df.head())\n",
    "\n",
    "print(\"Number of occurrences of features that are removed :\")\n",
    "df[\"X_removed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6sTQj5wDWdev"
   },
   "outputs": [],
   "source": [
    "#manually feature selection\n",
    "X_selected = []\n",
    "for i in range(len(X)):\n",
    "    #print(w[i][0])\n",
    "    X_selected.append([X[i][0],  X[i][5], X[i][6], X[i][7], X[i][8],\n",
    "               X[i][11], X[i][12], X[i][14], X[i][15], X[i][16], X[i][17],\n",
    "                X[i][18], X[i][19], X[i][21]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ch2KlT914uA9"
   },
   "source": [
    "Split dataset to Train, Test and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1596105551985,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "VYsXl_cV4vbq",
    "outputId": "48c1e15e-4b6a-4468-baa3-1d9695bd5cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093\n",
      "342\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "#split train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aN6WjeKMa8Y"
   },
   "source": [
    "Reshape Labels and features for deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFUFXgkLUQZp"
   },
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaaJCOWhTjcU"
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "X_val = np.asarray(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1596105744378,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "NbPd-wZjTBNq",
    "outputId": "a1c26aea-cf9a-4617-c662-11a2aa2996c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1093, 14, 1)\n",
      "(342, 14, 1)\n",
      "(274, 14, 1)\n"
     ]
    }
   ],
   "source": [
    " X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    " X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    " X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    " print(X_train.shape)\n",
    " print(X_test.shape)\n",
    " print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-unzcOMlUSc6"
   },
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1596106127538,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "5dXesYt5KsyA",
    "outputId": "5decfb6d-0fa5-483b-f146-12a2b414eea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1093, 2)\n",
      "(342, 2)\n",
      "(274, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8U62d8rGqo9"
   },
   "source": [
    "### Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XcJ-s24okEk"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goTNTktzg0L8"
   },
   "outputs": [],
   "source": [
    "input_y = Input(shape= (14,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1596106531238,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "objpwMFrPH6y",
    "outputId": "06fc93e2-e472-48a2-f35a-cb2e04b53f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 14, 1)]           0         \n",
      "_________________________________________________________________\n",
      "Conv_5 (Conv1D)              (None, 14, 128)           768       \n",
      "_________________________________________________________________\n",
      "BatchNorm_3 (BatchNormalizat (None, 14, 128)           512       \n",
      "_________________________________________________________________\n",
      "Activ_5 (Activation)         (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "Drop_2 (Dropout)             (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "Conv_6 (Conv1D)              (None, 14, 128)           82048     \n",
      "_________________________________________________________________\n",
      "Flat (Flatten)               (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "Drop_3 (Dropout)             (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 3586      \n",
      "_________________________________________________________________\n",
      "BatchNorm_4 (BatchNormalizat (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "Activ_6 (Activation)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 86,922\n",
      "Trainable params: 86,662\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## First LFLB (local feature learning block)\n",
    "y = Conv1D(256, kernel_size=5, strides= 1,  name='Conv_1', padding = 'same')(input_y)\n",
    "y = BatchNormalization( name='BatchNorm_1')(y)\n",
    "y = Activation('elu', name='Activ_1')(y)\n",
    "\n",
    "y = Conv1D(128, kernel_size=5, strides= 1,  name='Conv_2', padding = 'same')(input_y)\n",
    "y = Activation('elu', name='Activ_2')(y)\n",
    "\n",
    "y = Dropout(0.2, name='Drop_1')(y)\n",
    "y = BatchNormalization( name='BatchNorm_2')(y)\n",
    "y = MaxPooling1D(pool_size=8, name = 'maxpooling', padding = 'same') (y) \n",
    "\n",
    "y = Conv1D(128, kernel_size= 5, strides= 1,  name='Conv_3', padding = 'same')(y)\n",
    "y = Activation('elu', name='Activ_3')(y)\n",
    "\n",
    "y = Conv1D(128, kernel_size= 5, strides= 1,  name='Conv_4', padding = 'same')(y)\n",
    "y = Activation('elu', name='Activ_4')(y)\n",
    "\n",
    "y = Conv1D(128, kernel_size=5, strides= 1,  name='Conv_5', padding = 'same')(input_y)\n",
    "y = BatchNormalization( name='BatchNorm_3')(y)\n",
    "y = Activation('elu', name='Activ_5')(y)\n",
    "\n",
    "y = Dropout(0.2, name='Drop_2')(y)     \n",
    "\n",
    "y = Conv1D(128, kernel_size= 5, strides= 1,  name='Conv_6', padding = 'same')(y)\n",
    "\n",
    "y = Flatten(name='Flat')(y)\n",
    "y = Dropout(0.2, name='Drop_3')(y)  \n",
    "\n",
    "y = Dense(y_train.shape[1],  name='dense')(y)\n",
    "\n",
    "y = BatchNormalization(name='BatchNorm_4')(y)\n",
    "\n",
    "y = Activation('softmax', name='Activ_6')(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build final model\n",
    "model = Model(inputs=input_y, outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fl2GZEzYQBC0"
   },
   "outputs": [],
   "source": [
    "\n",
    "METRICS = [\n",
    "      \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      \n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192137,
     "status": "ok",
     "timestamp": 1596106770558,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "zHXRXbVTQEqd",
    "outputId": "5c53cbea-ee87-412a-ece4-da9b1521d806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.7360 - accuracy: 0.5801 - auc: 0.6121 - val_loss: 0.6825 - val_accuracy: 0.5702 - val_auc: 0.6123\n",
      "Epoch 2/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.6878 - accuracy: 0.6048 - auc: 0.6521 - val_loss: 0.6773 - val_accuracy: 0.5965 - val_auc: 0.6270\n",
      "Epoch 3/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.6711 - accuracy: 0.6258 - auc: 0.6795 - val_loss: 0.6737 - val_accuracy: 0.5965 - val_auc: 0.6314\n",
      "Epoch 4/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6440 - accuracy: 0.6478 - auc: 0.6978 - val_loss: 0.6699 - val_accuracy: 0.6140 - val_auc: 0.6413\n",
      "Epoch 5/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.6319 - accuracy: 0.6450 - auc: 0.7114 - val_loss: 0.6615 - val_accuracy: 0.6345 - val_auc: 0.6690\n",
      "Epoch 6/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.6481 - accuracy: 0.6478 - auc: 0.7036 - val_loss: 0.6541 - val_accuracy: 0.6608 - val_auc: 0.6936\n",
      "Epoch 7/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6119 - accuracy: 0.6770 - auc: 0.7326 - val_loss: 0.6475 - val_accuracy: 0.6696 - val_auc: 0.7061\n",
      "Epoch 8/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6153 - accuracy: 0.6715 - auc: 0.7344 - val_loss: 0.6417 - val_accuracy: 0.6637 - val_auc: 0.7174\n",
      "Epoch 9/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6163 - accuracy: 0.6651 - auc: 0.7321 - val_loss: 0.6369 - val_accuracy: 0.6784 - val_auc: 0.7247\n",
      "Epoch 10/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6204 - accuracy: 0.6743 - auc: 0.7310 - val_loss: 0.6306 - val_accuracy: 0.6784 - val_auc: 0.7343\n",
      "Epoch 11/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6012 - accuracy: 0.6743 - auc: 0.7464 - val_loss: 0.6230 - val_accuracy: 0.6959 - val_auc: 0.7462\n",
      "Epoch 12/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6105 - accuracy: 0.6706 - auc: 0.7383 - val_loss: 0.6196 - val_accuracy: 0.7076 - val_auc: 0.7495\n",
      "Epoch 13/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5949 - accuracy: 0.6871 - auc: 0.7503 - val_loss: 0.6148 - val_accuracy: 0.7135 - val_auc: 0.7573\n",
      "Epoch 14/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5918 - accuracy: 0.6825 - auc: 0.7532 - val_loss: 0.6087 - val_accuracy: 0.7135 - val_auc: 0.7638\n",
      "Epoch 15/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5989 - accuracy: 0.6697 - auc: 0.7451 - val_loss: 0.6032 - val_accuracy: 0.7105 - val_auc: 0.7699\n",
      "Epoch 16/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5986 - accuracy: 0.6880 - auc: 0.7478 - val_loss: 0.5984 - val_accuracy: 0.7105 - val_auc: 0.7733\n",
      "Epoch 17/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5895 - accuracy: 0.6798 - auc: 0.7509 - val_loss: 0.5952 - val_accuracy: 0.7164 - val_auc: 0.7754\n",
      "Epoch 18/700\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5972 - accuracy: 0.6780 - auc: 0.7437 - val_loss: 0.5927 - val_accuracy: 0.7222 - val_auc: 0.7760\n",
      "Epoch 19/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5860 - accuracy: 0.6834 - auc: 0.7560 - val_loss: 0.5860 - val_accuracy: 0.7193 - val_auc: 0.7800\n",
      "Epoch 20/700\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5868 - accuracy: 0.6926 - auc: 0.7566 - val_loss: 0.5817 - val_accuracy: 0.7164 - val_auc: 0.7827\n",
      "Epoch 21/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5871 - accuracy: 0.6789 - auc: 0.7529 - val_loss: 0.5756 - val_accuracy: 0.7193 - val_auc: 0.7864\n",
      "Epoch 22/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5852 - accuracy: 0.6871 - auc: 0.7603 - val_loss: 0.5717 - val_accuracy: 0.7193 - val_auc: 0.7883\n",
      "Epoch 23/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5888 - accuracy: 0.6871 - auc: 0.7516 - val_loss: 0.5688 - val_accuracy: 0.7135 - val_auc: 0.7894\n",
      "Epoch 24/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5830 - accuracy: 0.6962 - auc: 0.7600 - val_loss: 0.5652 - val_accuracy: 0.7164 - val_auc: 0.7921\n",
      "Epoch 25/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5783 - accuracy: 0.6834 - auc: 0.7621 - val_loss: 0.5626 - val_accuracy: 0.7135 - val_auc: 0.7925\n",
      "Epoch 26/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5760 - accuracy: 0.6898 - auc: 0.7642 - val_loss: 0.5589 - val_accuracy: 0.7222 - val_auc: 0.7951\n",
      "Epoch 27/700\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5678 - accuracy: 0.6990 - auc: 0.7735 - val_loss: 0.5584 - val_accuracy: 0.7164 - val_auc: 0.7952\n",
      "Epoch 28/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5797 - accuracy: 0.6908 - auc: 0.7619 - val_loss: 0.5571 - val_accuracy: 0.7105 - val_auc: 0.7950\n",
      "Epoch 29/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5757 - accuracy: 0.6926 - auc: 0.7664 - val_loss: 0.5552 - val_accuracy: 0.7193 - val_auc: 0.7956\n",
      "Epoch 30/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5813 - accuracy: 0.6862 - auc: 0.7596 - val_loss: 0.5515 - val_accuracy: 0.7193 - val_auc: 0.7984\n",
      "Epoch 31/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5738 - accuracy: 0.7045 - auc: 0.7679 - val_loss: 0.5512 - val_accuracy: 0.7251 - val_auc: 0.7980\n",
      "Epoch 32/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5788 - accuracy: 0.6871 - auc: 0.7632 - val_loss: 0.5491 - val_accuracy: 0.7193 - val_auc: 0.7997\n",
      "Epoch 33/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5835 - accuracy: 0.6834 - auc: 0.7569 - val_loss: 0.5472 - val_accuracy: 0.7222 - val_auc: 0.8011\n",
      "Epoch 34/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5721 - accuracy: 0.6889 - auc: 0.7690 - val_loss: 0.5469 - val_accuracy: 0.7193 - val_auc: 0.8017\n",
      "Epoch 35/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5770 - accuracy: 0.6844 - auc: 0.7646 - val_loss: 0.5467 - val_accuracy: 0.7193 - val_auc: 0.8011\n",
      "Epoch 36/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5661 - accuracy: 0.7045 - auc: 0.7768 - val_loss: 0.5457 - val_accuracy: 0.7164 - val_auc: 0.8016\n",
      "Epoch 37/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5786 - accuracy: 0.6953 - auc: 0.7659 - val_loss: 0.5449 - val_accuracy: 0.7164 - val_auc: 0.8021\n",
      "Epoch 38/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5703 - accuracy: 0.6972 - auc: 0.7695 - val_loss: 0.5445 - val_accuracy: 0.7164 - val_auc: 0.8025\n",
      "Epoch 39/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5726 - accuracy: 0.7017 - auc: 0.7686 - val_loss: 0.5433 - val_accuracy: 0.7164 - val_auc: 0.8032\n",
      "Epoch 40/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5611 - accuracy: 0.6972 - auc: 0.7789 - val_loss: 0.5427 - val_accuracy: 0.7193 - val_auc: 0.8038\n",
      "Epoch 41/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5709 - accuracy: 0.7164 - auc: 0.7737 - val_loss: 0.5426 - val_accuracy: 0.7222 - val_auc: 0.8042\n",
      "Epoch 42/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5748 - accuracy: 0.6789 - auc: 0.7656 - val_loss: 0.5429 - val_accuracy: 0.7222 - val_auc: 0.8034\n",
      "Epoch 43/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5708 - accuracy: 0.7008 - auc: 0.7712 - val_loss: 0.5427 - val_accuracy: 0.7222 - val_auc: 0.8039\n",
      "Epoch 44/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5708 - accuracy: 0.6953 - auc: 0.7716 - val_loss: 0.5425 - val_accuracy: 0.7251 - val_auc: 0.8043\n",
      "Epoch 45/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5679 - accuracy: 0.6972 - auc: 0.7745 - val_loss: 0.5405 - val_accuracy: 0.7222 - val_auc: 0.8054\n",
      "Epoch 46/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5691 - accuracy: 0.7008 - auc: 0.7711 - val_loss: 0.5409 - val_accuracy: 0.7222 - val_auc: 0.8050\n",
      "Epoch 47/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5706 - accuracy: 0.7036 - auc: 0.7711 - val_loss: 0.5397 - val_accuracy: 0.7193 - val_auc: 0.8057\n",
      "Epoch 48/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5755 - accuracy: 0.7063 - auc: 0.7678 - val_loss: 0.5414 - val_accuracy: 0.7251 - val_auc: 0.8048\n",
      "Epoch 49/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5713 - accuracy: 0.6880 - auc: 0.7700 - val_loss: 0.5420 - val_accuracy: 0.7310 - val_auc: 0.8045\n",
      "Epoch 50/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5768 - accuracy: 0.7027 - auc: 0.7653 - val_loss: 0.5395 - val_accuracy: 0.7251 - val_auc: 0.8055\n",
      "Epoch 51/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5670 - accuracy: 0.6871 - auc: 0.7730 - val_loss: 0.5377 - val_accuracy: 0.7222 - val_auc: 0.8072\n",
      "Epoch 52/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5667 - accuracy: 0.7091 - auc: 0.7746 - val_loss: 0.5384 - val_accuracy: 0.7251 - val_auc: 0.8067\n",
      "Epoch 53/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5623 - accuracy: 0.7109 - auc: 0.7807 - val_loss: 0.5390 - val_accuracy: 0.7193 - val_auc: 0.8067\n",
      "Epoch 54/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5689 - accuracy: 0.6972 - auc: 0.7727 - val_loss: 0.5388 - val_accuracy: 0.7222 - val_auc: 0.8063\n",
      "Epoch 55/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5748 - accuracy: 0.7054 - auc: 0.7678 - val_loss: 0.5383 - val_accuracy: 0.7193 - val_auc: 0.8070\n",
      "Epoch 56/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5628 - accuracy: 0.7027 - auc: 0.7775 - val_loss: 0.5378 - val_accuracy: 0.7222 - val_auc: 0.8071\n",
      "Epoch 57/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5739 - accuracy: 0.6889 - auc: 0.7671 - val_loss: 0.5374 - val_accuracy: 0.7193 - val_auc: 0.8076\n",
      "Epoch 58/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5698 - accuracy: 0.6990 - auc: 0.7727 - val_loss: 0.5377 - val_accuracy: 0.7251 - val_auc: 0.8079\n",
      "Epoch 59/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5651 - accuracy: 0.7027 - auc: 0.7756 - val_loss: 0.5385 - val_accuracy: 0.7281 - val_auc: 0.8067\n",
      "Epoch 60/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5610 - accuracy: 0.7127 - auc: 0.7817 - val_loss: 0.5392 - val_accuracy: 0.7398 - val_auc: 0.8063\n",
      "Epoch 61/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5667 - accuracy: 0.6889 - auc: 0.7749 - val_loss: 0.5379 - val_accuracy: 0.7339 - val_auc: 0.8076\n",
      "Epoch 62/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5601 - accuracy: 0.6962 - auc: 0.7828 - val_loss: 0.5364 - val_accuracy: 0.7368 - val_auc: 0.8085\n",
      "Epoch 63/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5623 - accuracy: 0.7036 - auc: 0.7798 - val_loss: 0.5355 - val_accuracy: 0.7310 - val_auc: 0.8090\n",
      "Epoch 64/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5658 - accuracy: 0.7155 - auc: 0.7765 - val_loss: 0.5353 - val_accuracy: 0.7339 - val_auc: 0.8094\n",
      "Epoch 65/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5697 - accuracy: 0.6917 - auc: 0.7699 - val_loss: 0.5355 - val_accuracy: 0.7339 - val_auc: 0.8094\n",
      "Epoch 66/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5577 - accuracy: 0.7136 - auc: 0.7849 - val_loss: 0.5349 - val_accuracy: 0.7310 - val_auc: 0.8105\n",
      "Epoch 67/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5620 - accuracy: 0.7072 - auc: 0.7801 - val_loss: 0.5357 - val_accuracy: 0.7310 - val_auc: 0.8098\n",
      "Epoch 68/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5603 - accuracy: 0.7191 - auc: 0.7834 - val_loss: 0.5351 - val_accuracy: 0.7310 - val_auc: 0.8098\n",
      "Epoch 69/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5540 - accuracy: 0.7100 - auc: 0.7860 - val_loss: 0.5341 - val_accuracy: 0.7339 - val_auc: 0.8107\n",
      "Epoch 70/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5601 - accuracy: 0.7054 - auc: 0.7798 - val_loss: 0.5348 - val_accuracy: 0.7310 - val_auc: 0.8098\n",
      "Epoch 71/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5627 - accuracy: 0.7017 - auc: 0.7789 - val_loss: 0.5348 - val_accuracy: 0.7310 - val_auc: 0.8100\n",
      "Epoch 72/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5568 - accuracy: 0.7091 - auc: 0.7871 - val_loss: 0.5355 - val_accuracy: 0.7281 - val_auc: 0.8097\n",
      "Epoch 73/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5648 - accuracy: 0.6999 - auc: 0.7765 - val_loss: 0.5352 - val_accuracy: 0.7310 - val_auc: 0.8098\n",
      "Epoch 74/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5606 - accuracy: 0.6944 - auc: 0.7793 - val_loss: 0.5354 - val_accuracy: 0.7339 - val_auc: 0.8097\n",
      "Epoch 75/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5559 - accuracy: 0.6990 - auc: 0.7834 - val_loss: 0.5337 - val_accuracy: 0.7368 - val_auc: 0.8114\n",
      "Epoch 76/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5660 - accuracy: 0.7081 - auc: 0.7758 - val_loss: 0.5339 - val_accuracy: 0.7339 - val_auc: 0.8112\n",
      "Epoch 77/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5649 - accuracy: 0.6908 - auc: 0.7760 - val_loss: 0.5328 - val_accuracy: 0.7339 - val_auc: 0.8119\n",
      "Epoch 78/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5574 - accuracy: 0.7008 - auc: 0.7843 - val_loss: 0.5344 - val_accuracy: 0.7310 - val_auc: 0.8107\n",
      "Epoch 79/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5549 - accuracy: 0.7063 - auc: 0.7837 - val_loss: 0.5344 - val_accuracy: 0.7398 - val_auc: 0.8111\n",
      "Epoch 80/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5570 - accuracy: 0.7072 - auc: 0.7849 - val_loss: 0.5328 - val_accuracy: 0.7398 - val_auc: 0.8120\n",
      "Epoch 81/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5645 - accuracy: 0.6908 - auc: 0.7775 - val_loss: 0.5337 - val_accuracy: 0.7427 - val_auc: 0.8112\n",
      "Epoch 82/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5590 - accuracy: 0.6944 - auc: 0.7825 - val_loss: 0.5335 - val_accuracy: 0.7427 - val_auc: 0.8115\n",
      "Epoch 83/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5568 - accuracy: 0.7081 - auc: 0.7860 - val_loss: 0.5342 - val_accuracy: 0.7398 - val_auc: 0.8105\n",
      "Epoch 84/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5563 - accuracy: 0.7091 - auc: 0.7854 - val_loss: 0.5341 - val_accuracy: 0.7427 - val_auc: 0.8108\n",
      "Epoch 85/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5522 - accuracy: 0.7091 - auc: 0.7894 - val_loss: 0.5341 - val_accuracy: 0.7398 - val_auc: 0.8105\n",
      "Epoch 86/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5536 - accuracy: 0.7100 - auc: 0.7886 - val_loss: 0.5325 - val_accuracy: 0.7456 - val_auc: 0.8120\n",
      "Epoch 87/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5580 - accuracy: 0.7027 - auc: 0.7814 - val_loss: 0.5326 - val_accuracy: 0.7427 - val_auc: 0.8119\n",
      "Epoch 88/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5473 - accuracy: 0.7136 - auc: 0.7944 - val_loss: 0.5332 - val_accuracy: 0.7485 - val_auc: 0.8119\n",
      "Epoch 89/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5581 - accuracy: 0.7136 - auc: 0.7836 - val_loss: 0.5332 - val_accuracy: 0.7427 - val_auc: 0.8117\n",
      "Epoch 90/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5538 - accuracy: 0.7017 - auc: 0.7869 - val_loss: 0.5333 - val_accuracy: 0.7368 - val_auc: 0.8113\n",
      "Epoch 91/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5571 - accuracy: 0.6990 - auc: 0.7826 - val_loss: 0.5326 - val_accuracy: 0.7368 - val_auc: 0.8116\n",
      "Epoch 92/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5570 - accuracy: 0.7027 - auc: 0.7841 - val_loss: 0.5330 - val_accuracy: 0.7368 - val_auc: 0.8114\n",
      "Epoch 93/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5533 - accuracy: 0.7063 - auc: 0.7872 - val_loss: 0.5337 - val_accuracy: 0.7398 - val_auc: 0.8111\n",
      "Epoch 94/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5550 - accuracy: 0.6926 - auc: 0.7845 - val_loss: 0.5324 - val_accuracy: 0.7398 - val_auc: 0.8120\n",
      "Epoch 95/700\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5554 - accuracy: 0.7036 - auc: 0.7852 - val_loss: 0.5318 - val_accuracy: 0.7398 - val_auc: 0.8123\n",
      "Epoch 96/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5522 - accuracy: 0.7072 - auc: 0.7883 - val_loss: 0.5315 - val_accuracy: 0.7368 - val_auc: 0.8125\n",
      "Epoch 97/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5550 - accuracy: 0.7008 - auc: 0.7865 - val_loss: 0.5306 - val_accuracy: 0.7368 - val_auc: 0.8132\n",
      "Epoch 98/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5519 - accuracy: 0.7145 - auc: 0.7892 - val_loss: 0.5301 - val_accuracy: 0.7368 - val_auc: 0.8139\n",
      "Epoch 99/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5482 - accuracy: 0.7027 - auc: 0.7909 - val_loss: 0.5311 - val_accuracy: 0.7368 - val_auc: 0.8132\n",
      "Epoch 100/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5488 - accuracy: 0.7118 - auc: 0.7905 - val_loss: 0.5311 - val_accuracy: 0.7398 - val_auc: 0.8131\n",
      "Epoch 101/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5626 - accuracy: 0.6953 - auc: 0.7772 - val_loss: 0.5308 - val_accuracy: 0.7485 - val_auc: 0.8132\n",
      "Epoch 102/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5492 - accuracy: 0.7164 - auc: 0.7918 - val_loss: 0.5293 - val_accuracy: 0.7485 - val_auc: 0.8142\n",
      "Epoch 103/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5409 - accuracy: 0.7008 - auc: 0.7959 - val_loss: 0.5284 - val_accuracy: 0.7485 - val_auc: 0.8146\n",
      "Epoch 104/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5453 - accuracy: 0.7136 - auc: 0.7960 - val_loss: 0.5293 - val_accuracy: 0.7485 - val_auc: 0.8145\n",
      "Epoch 105/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5395 - accuracy: 0.7210 - auc: 0.8012 - val_loss: 0.5283 - val_accuracy: 0.7485 - val_auc: 0.8152\n",
      "Epoch 106/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5545 - accuracy: 0.7008 - auc: 0.7850 - val_loss: 0.5299 - val_accuracy: 0.7427 - val_auc: 0.8136\n",
      "Epoch 107/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5573 - accuracy: 0.6953 - auc: 0.7819 - val_loss: 0.5295 - val_accuracy: 0.7427 - val_auc: 0.8140\n",
      "Epoch 108/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5474 - accuracy: 0.6981 - auc: 0.7920 - val_loss: 0.5301 - val_accuracy: 0.7427 - val_auc: 0.8137\n",
      "Epoch 109/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5444 - accuracy: 0.7118 - auc: 0.7967 - val_loss: 0.5307 - val_accuracy: 0.7427 - val_auc: 0.8128\n",
      "Epoch 110/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5541 - accuracy: 0.7027 - auc: 0.7868 - val_loss: 0.5294 - val_accuracy: 0.7485 - val_auc: 0.8137\n",
      "Epoch 111/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5456 - accuracy: 0.7145 - auc: 0.7960 - val_loss: 0.5285 - val_accuracy: 0.7456 - val_auc: 0.8144\n",
      "Epoch 112/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5543 - accuracy: 0.6962 - auc: 0.7850 - val_loss: 0.5293 - val_accuracy: 0.7485 - val_auc: 0.8140\n",
      "Epoch 113/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5533 - accuracy: 0.7017 - auc: 0.7877 - val_loss: 0.5276 - val_accuracy: 0.7456 - val_auc: 0.8151\n",
      "Epoch 114/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5436 - accuracy: 0.7127 - auc: 0.7970 - val_loss: 0.5280 - val_accuracy: 0.7515 - val_auc: 0.8148\n",
      "Epoch 115/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5527 - accuracy: 0.7091 - auc: 0.7865 - val_loss: 0.5266 - val_accuracy: 0.7485 - val_auc: 0.8160\n",
      "Epoch 116/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5497 - accuracy: 0.7164 - auc: 0.7895 - val_loss: 0.5277 - val_accuracy: 0.7485 - val_auc: 0.8153\n",
      "Epoch 117/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5442 - accuracy: 0.7063 - auc: 0.7959 - val_loss: 0.5263 - val_accuracy: 0.7544 - val_auc: 0.8163\n",
      "Epoch 118/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5509 - accuracy: 0.7155 - auc: 0.7900 - val_loss: 0.5266 - val_accuracy: 0.7515 - val_auc: 0.8170\n",
      "Epoch 119/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5376 - accuracy: 0.7219 - auc: 0.8046 - val_loss: 0.5258 - val_accuracy: 0.7485 - val_auc: 0.8170\n",
      "Epoch 120/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5483 - accuracy: 0.7109 - auc: 0.7915 - val_loss: 0.5256 - val_accuracy: 0.7485 - val_auc: 0.8169\n",
      "Epoch 121/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5448 - accuracy: 0.7164 - auc: 0.7954 - val_loss: 0.5257 - val_accuracy: 0.7456 - val_auc: 0.8169\n",
      "Epoch 122/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5472 - accuracy: 0.7191 - auc: 0.7949 - val_loss: 0.5259 - val_accuracy: 0.7456 - val_auc: 0.8172\n",
      "Epoch 123/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5553 - accuracy: 0.7063 - auc: 0.7851 - val_loss: 0.5248 - val_accuracy: 0.7456 - val_auc: 0.8177\n",
      "Epoch 124/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5458 - accuracy: 0.7164 - auc: 0.7962 - val_loss: 0.5245 - val_accuracy: 0.7515 - val_auc: 0.8174\n",
      "Epoch 125/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5600 - accuracy: 0.6999 - auc: 0.7795 - val_loss: 0.5228 - val_accuracy: 0.7485 - val_auc: 0.8189\n",
      "Epoch 126/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5490 - accuracy: 0.7008 - auc: 0.7893 - val_loss: 0.5225 - val_accuracy: 0.7544 - val_auc: 0.8191\n",
      "Epoch 127/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5553 - accuracy: 0.7081 - auc: 0.7849 - val_loss: 0.5237 - val_accuracy: 0.7515 - val_auc: 0.8183\n",
      "Epoch 128/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5524 - accuracy: 0.7072 - auc: 0.7889 - val_loss: 0.5241 - val_accuracy: 0.7515 - val_auc: 0.8184\n",
      "Epoch 129/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5497 - accuracy: 0.7008 - auc: 0.7895 - val_loss: 0.5249 - val_accuracy: 0.7485 - val_auc: 0.8177\n",
      "Epoch 130/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5487 - accuracy: 0.7118 - auc: 0.7929 - val_loss: 0.5246 - val_accuracy: 0.7485 - val_auc: 0.8180\n",
      "Epoch 131/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5466 - accuracy: 0.7182 - auc: 0.7944 - val_loss: 0.5249 - val_accuracy: 0.7515 - val_auc: 0.8173\n",
      "Epoch 132/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5489 - accuracy: 0.7173 - auc: 0.7925 - val_loss: 0.5241 - val_accuracy: 0.7573 - val_auc: 0.8187\n",
      "Epoch 133/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5495 - accuracy: 0.6889 - auc: 0.7911 - val_loss: 0.5257 - val_accuracy: 0.7544 - val_auc: 0.8170\n",
      "Epoch 134/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5564 - accuracy: 0.7109 - auc: 0.7860 - val_loss: 0.5258 - val_accuracy: 0.7544 - val_auc: 0.8164\n",
      "Epoch 135/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5529 - accuracy: 0.7072 - auc: 0.7894 - val_loss: 0.5250 - val_accuracy: 0.7515 - val_auc: 0.8174\n",
      "Epoch 136/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5517 - accuracy: 0.7145 - auc: 0.7882 - val_loss: 0.5235 - val_accuracy: 0.7485 - val_auc: 0.8183\n",
      "Epoch 137/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5382 - accuracy: 0.7210 - auc: 0.8031 - val_loss: 0.5236 - val_accuracy: 0.7515 - val_auc: 0.8185\n",
      "Epoch 138/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5518 - accuracy: 0.7054 - auc: 0.7898 - val_loss: 0.5233 - val_accuracy: 0.7515 - val_auc: 0.8184\n",
      "Epoch 139/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5493 - accuracy: 0.7045 - auc: 0.7892 - val_loss: 0.5214 - val_accuracy: 0.7485 - val_auc: 0.8200\n",
      "Epoch 140/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5404 - accuracy: 0.7200 - auc: 0.8000 - val_loss: 0.5213 - val_accuracy: 0.7515 - val_auc: 0.8200\n",
      "Epoch 141/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5427 - accuracy: 0.7191 - auc: 0.7993 - val_loss: 0.5214 - val_accuracy: 0.7485 - val_auc: 0.8197\n",
      "Epoch 142/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5351 - accuracy: 0.7182 - auc: 0.8063 - val_loss: 0.5232 - val_accuracy: 0.7485 - val_auc: 0.8187\n",
      "Epoch 143/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5427 - accuracy: 0.7164 - auc: 0.7962 - val_loss: 0.5247 - val_accuracy: 0.7485 - val_auc: 0.8173\n",
      "Epoch 144/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5408 - accuracy: 0.7200 - auc: 0.8007 - val_loss: 0.5231 - val_accuracy: 0.7515 - val_auc: 0.8187\n",
      "Epoch 145/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5482 - accuracy: 0.7145 - auc: 0.7924 - val_loss: 0.5235 - val_accuracy: 0.7544 - val_auc: 0.8184\n",
      "Epoch 146/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5331 - accuracy: 0.7228 - auc: 0.8063 - val_loss: 0.5234 - val_accuracy: 0.7544 - val_auc: 0.8184\n",
      "Epoch 147/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5512 - accuracy: 0.7127 - auc: 0.7915 - val_loss: 0.5235 - val_accuracy: 0.7544 - val_auc: 0.8185\n",
      "Epoch 148/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5437 - accuracy: 0.7027 - auc: 0.7976 - val_loss: 0.5229 - val_accuracy: 0.7573 - val_auc: 0.8189\n",
      "Epoch 149/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5385 - accuracy: 0.7136 - auc: 0.7988 - val_loss: 0.5234 - val_accuracy: 0.7602 - val_auc: 0.8191\n",
      "Epoch 150/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5362 - accuracy: 0.7164 - auc: 0.8028 - val_loss: 0.5241 - val_accuracy: 0.7544 - val_auc: 0.8182\n",
      "Epoch 151/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5422 - accuracy: 0.7036 - auc: 0.7965 - val_loss: 0.5237 - val_accuracy: 0.7632 - val_auc: 0.8182\n",
      "Epoch 152/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5405 - accuracy: 0.7164 - auc: 0.7993 - val_loss: 0.5231 - val_accuracy: 0.7544 - val_auc: 0.8188\n",
      "Epoch 153/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5491 - accuracy: 0.7109 - auc: 0.7920 - val_loss: 0.5223 - val_accuracy: 0.7544 - val_auc: 0.8196\n",
      "Epoch 154/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5411 - accuracy: 0.7292 - auc: 0.8008 - val_loss: 0.5222 - val_accuracy: 0.7544 - val_auc: 0.8196\n",
      "Epoch 155/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5460 - accuracy: 0.7145 - auc: 0.7946 - val_loss: 0.5212 - val_accuracy: 0.7544 - val_auc: 0.8206\n",
      "Epoch 156/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5446 - accuracy: 0.7155 - auc: 0.7960 - val_loss: 0.5228 - val_accuracy: 0.7515 - val_auc: 0.8197\n",
      "Epoch 157/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5354 - accuracy: 0.7301 - auc: 0.8043 - val_loss: 0.5227 - val_accuracy: 0.7515 - val_auc: 0.8192\n",
      "Epoch 158/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5422 - accuracy: 0.7155 - auc: 0.7995 - val_loss: 0.5228 - val_accuracy: 0.7515 - val_auc: 0.8189\n",
      "Epoch 159/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5386 - accuracy: 0.7237 - auc: 0.8016 - val_loss: 0.5223 - val_accuracy: 0.7515 - val_auc: 0.8193\n",
      "Epoch 160/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5405 - accuracy: 0.7200 - auc: 0.8008 - val_loss: 0.5248 - val_accuracy: 0.7485 - val_auc: 0.8177\n",
      "Epoch 161/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5360 - accuracy: 0.7182 - auc: 0.8041 - val_loss: 0.5248 - val_accuracy: 0.7544 - val_auc: 0.8178\n",
      "Epoch 162/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5361 - accuracy: 0.7173 - auc: 0.8030 - val_loss: 0.5223 - val_accuracy: 0.7515 - val_auc: 0.8204\n",
      "Epoch 163/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5378 - accuracy: 0.7136 - auc: 0.8013 - val_loss: 0.5227 - val_accuracy: 0.7515 - val_auc: 0.8199\n",
      "Epoch 164/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5462 - accuracy: 0.7145 - auc: 0.7947 - val_loss: 0.5226 - val_accuracy: 0.7515 - val_auc: 0.8199\n",
      "Epoch 165/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5444 - accuracy: 0.7109 - auc: 0.7955 - val_loss: 0.5220 - val_accuracy: 0.7515 - val_auc: 0.8202\n",
      "Epoch 166/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5402 - accuracy: 0.7200 - auc: 0.7997 - val_loss: 0.5209 - val_accuracy: 0.7544 - val_auc: 0.8207\n",
      "Epoch 167/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5280 - accuracy: 0.7237 - auc: 0.8107 - val_loss: 0.5201 - val_accuracy: 0.7544 - val_auc: 0.8216\n",
      "Epoch 168/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5452 - accuracy: 0.7255 - auc: 0.7963 - val_loss: 0.5214 - val_accuracy: 0.7544 - val_auc: 0.8206\n",
      "Epoch 169/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5417 - accuracy: 0.7292 - auc: 0.7991 - val_loss: 0.5208 - val_accuracy: 0.7544 - val_auc: 0.8214\n",
      "Epoch 170/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5449 - accuracy: 0.7246 - auc: 0.7987 - val_loss: 0.5188 - val_accuracy: 0.7544 - val_auc: 0.8224\n",
      "Epoch 171/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5360 - accuracy: 0.7136 - auc: 0.8022 - val_loss: 0.5195 - val_accuracy: 0.7544 - val_auc: 0.8220\n",
      "Epoch 172/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5377 - accuracy: 0.7164 - auc: 0.7993 - val_loss: 0.5203 - val_accuracy: 0.7515 - val_auc: 0.8214\n",
      "Epoch 173/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5506 - accuracy: 0.6926 - auc: 0.7877 - val_loss: 0.5184 - val_accuracy: 0.7573 - val_auc: 0.8228\n",
      "Epoch 174/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5473 - accuracy: 0.7063 - auc: 0.7915 - val_loss: 0.5189 - val_accuracy: 0.7573 - val_auc: 0.8220\n",
      "Epoch 175/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5438 - accuracy: 0.7237 - auc: 0.7985 - val_loss: 0.5179 - val_accuracy: 0.7544 - val_auc: 0.8229\n",
      "Epoch 176/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5315 - accuracy: 0.7392 - auc: 0.8099 - val_loss: 0.5177 - val_accuracy: 0.7573 - val_auc: 0.8229\n",
      "Epoch 177/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5411 - accuracy: 0.7301 - auc: 0.7992 - val_loss: 0.5179 - val_accuracy: 0.7573 - val_auc: 0.8232\n",
      "Epoch 178/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5430 - accuracy: 0.7136 - auc: 0.7964 - val_loss: 0.5189 - val_accuracy: 0.7544 - val_auc: 0.8224\n",
      "Epoch 179/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5461 - accuracy: 0.7173 - auc: 0.7956 - val_loss: 0.5186 - val_accuracy: 0.7544 - val_auc: 0.8224\n",
      "Epoch 180/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5473 - accuracy: 0.7091 - auc: 0.7939 - val_loss: 0.5175 - val_accuracy: 0.7544 - val_auc: 0.8234\n",
      "Epoch 181/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5360 - accuracy: 0.7155 - auc: 0.8033 - val_loss: 0.5190 - val_accuracy: 0.7544 - val_auc: 0.8220\n",
      "Epoch 182/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5394 - accuracy: 0.7182 - auc: 0.8006 - val_loss: 0.5200 - val_accuracy: 0.7544 - val_auc: 0.8214\n",
      "Epoch 183/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5419 - accuracy: 0.7109 - auc: 0.7968 - val_loss: 0.5204 - val_accuracy: 0.7515 - val_auc: 0.8217\n",
      "Epoch 184/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5329 - accuracy: 0.7173 - auc: 0.8041 - val_loss: 0.5195 - val_accuracy: 0.7573 - val_auc: 0.8224\n",
      "Epoch 185/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5490 - accuracy: 0.7145 - auc: 0.7926 - val_loss: 0.5207 - val_accuracy: 0.7573 - val_auc: 0.8210\n",
      "Epoch 186/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5429 - accuracy: 0.7091 - auc: 0.7973 - val_loss: 0.5203 - val_accuracy: 0.7544 - val_auc: 0.8214\n",
      "Epoch 187/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5411 - accuracy: 0.7164 - auc: 0.7985 - val_loss: 0.5198 - val_accuracy: 0.7544 - val_auc: 0.8222\n",
      "Epoch 188/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5313 - accuracy: 0.7283 - auc: 0.8085 - val_loss: 0.5195 - val_accuracy: 0.7573 - val_auc: 0.8221\n",
      "Epoch 189/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5336 - accuracy: 0.7301 - auc: 0.8062 - val_loss: 0.5204 - val_accuracy: 0.7573 - val_auc: 0.8219\n",
      "Epoch 190/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5376 - accuracy: 0.7246 - auc: 0.8029 - val_loss: 0.5190 - val_accuracy: 0.7602 - val_auc: 0.8226\n",
      "Epoch 191/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5302 - accuracy: 0.7246 - auc: 0.8105 - val_loss: 0.5190 - val_accuracy: 0.7544 - val_auc: 0.8223\n",
      "Epoch 192/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5426 - accuracy: 0.7164 - auc: 0.7976 - val_loss: 0.5202 - val_accuracy: 0.7544 - val_auc: 0.8216\n",
      "Epoch 193/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5387 - accuracy: 0.7255 - auc: 0.8001 - val_loss: 0.5199 - val_accuracy: 0.7544 - val_auc: 0.8219\n",
      "Epoch 194/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5247 - accuracy: 0.7374 - auc: 0.8162 - val_loss: 0.5194 - val_accuracy: 0.7544 - val_auc: 0.8227\n",
      "Epoch 195/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5398 - accuracy: 0.7191 - auc: 0.7996 - val_loss: 0.5194 - val_accuracy: 0.7573 - val_auc: 0.8224\n",
      "Epoch 196/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5340 - accuracy: 0.7274 - auc: 0.8069 - val_loss: 0.5182 - val_accuracy: 0.7573 - val_auc: 0.8233\n",
      "Epoch 197/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5463 - accuracy: 0.7045 - auc: 0.7935 - val_loss: 0.5180 - val_accuracy: 0.7573 - val_auc: 0.8230\n",
      "Epoch 198/700\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5357 - accuracy: 0.7100 - auc: 0.8045 - val_loss: 0.5175 - val_accuracy: 0.7544 - val_auc: 0.8233\n",
      "Epoch 199/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5384 - accuracy: 0.7182 - auc: 0.8010 - val_loss: 0.5172 - val_accuracy: 0.7573 - val_auc: 0.8233\n",
      "Epoch 200/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5373 - accuracy: 0.7182 - auc: 0.8023 - val_loss: 0.5183 - val_accuracy: 0.7573 - val_auc: 0.8229\n",
      "Epoch 201/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5363 - accuracy: 0.7191 - auc: 0.8043 - val_loss: 0.5192 - val_accuracy: 0.7544 - val_auc: 0.8223\n",
      "Epoch 202/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5395 - accuracy: 0.7200 - auc: 0.8012 - val_loss: 0.5186 - val_accuracy: 0.7573 - val_auc: 0.8229\n",
      "Epoch 203/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5364 - accuracy: 0.7200 - auc: 0.8046 - val_loss: 0.5187 - val_accuracy: 0.7544 - val_auc: 0.8227\n",
      "Epoch 204/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5350 - accuracy: 0.7219 - auc: 0.8042 - val_loss: 0.5196 - val_accuracy: 0.7544 - val_auc: 0.8224\n",
      "Epoch 205/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5395 - accuracy: 0.7246 - auc: 0.8019 - val_loss: 0.5206 - val_accuracy: 0.7544 - val_auc: 0.8214\n",
      "Epoch 206/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5414 - accuracy: 0.7173 - auc: 0.7990 - val_loss: 0.5210 - val_accuracy: 0.7544 - val_auc: 0.8207\n",
      "Epoch 207/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5364 - accuracy: 0.7191 - auc: 0.8033 - val_loss: 0.5187 - val_accuracy: 0.7573 - val_auc: 0.8231\n",
      "Epoch 208/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5327 - accuracy: 0.7155 - auc: 0.8081 - val_loss: 0.5172 - val_accuracy: 0.7573 - val_auc: 0.8241\n",
      "Epoch 209/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5338 - accuracy: 0.7210 - auc: 0.8041 - val_loss: 0.5175 - val_accuracy: 0.7573 - val_auc: 0.8239\n",
      "Epoch 210/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5278 - accuracy: 0.7328 - auc: 0.8130 - val_loss: 0.5167 - val_accuracy: 0.7602 - val_auc: 0.8247\n",
      "Epoch 211/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5400 - accuracy: 0.7136 - auc: 0.8004 - val_loss: 0.5167 - val_accuracy: 0.7632 - val_auc: 0.8247\n",
      "Epoch 212/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5371 - accuracy: 0.7283 - auc: 0.8032 - val_loss: 0.5180 - val_accuracy: 0.7602 - val_auc: 0.8238\n",
      "Epoch 213/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5339 - accuracy: 0.7283 - auc: 0.8079 - val_loss: 0.5173 - val_accuracy: 0.7632 - val_auc: 0.8247\n",
      "Epoch 214/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5443 - accuracy: 0.7127 - auc: 0.7948 - val_loss: 0.5171 - val_accuracy: 0.7632 - val_auc: 0.8247\n",
      "Epoch 215/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5332 - accuracy: 0.7228 - auc: 0.8056 - val_loss: 0.5146 - val_accuracy: 0.7690 - val_auc: 0.8265\n",
      "Epoch 216/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5320 - accuracy: 0.7228 - auc: 0.8071 - val_loss: 0.5146 - val_accuracy: 0.7632 - val_auc: 0.8264\n",
      "Epoch 217/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5423 - accuracy: 0.7255 - auc: 0.7998 - val_loss: 0.5149 - val_accuracy: 0.7632 - val_auc: 0.8261\n",
      "Epoch 218/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5246 - accuracy: 0.7283 - auc: 0.8145 - val_loss: 0.5169 - val_accuracy: 0.7661 - val_auc: 0.8245\n",
      "Epoch 219/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5270 - accuracy: 0.7228 - auc: 0.8101 - val_loss: 0.5162 - val_accuracy: 0.7661 - val_auc: 0.8251\n",
      "Epoch 220/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5265 - accuracy: 0.7429 - auc: 0.8147 - val_loss: 0.5161 - val_accuracy: 0.7661 - val_auc: 0.8253\n",
      "Epoch 221/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5430 - accuracy: 0.7027 - auc: 0.7963 - val_loss: 0.5155 - val_accuracy: 0.7632 - val_auc: 0.8259\n",
      "Epoch 222/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5353 - accuracy: 0.7237 - auc: 0.8025 - val_loss: 0.5150 - val_accuracy: 0.7602 - val_auc: 0.8256\n",
      "Epoch 223/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5275 - accuracy: 0.7228 - auc: 0.8109 - val_loss: 0.5140 - val_accuracy: 0.7602 - val_auc: 0.8263\n",
      "Epoch 224/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5278 - accuracy: 0.7365 - auc: 0.8122 - val_loss: 0.5143 - val_accuracy: 0.7573 - val_auc: 0.8260\n",
      "Epoch 225/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5379 - accuracy: 0.7237 - auc: 0.8022 - val_loss: 0.5133 - val_accuracy: 0.7602 - val_auc: 0.8265\n",
      "Epoch 226/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5223 - accuracy: 0.7383 - auc: 0.8160 - val_loss: 0.5132 - val_accuracy: 0.7602 - val_auc: 0.8264\n",
      "Epoch 227/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5293 - accuracy: 0.7347 - auc: 0.8087 - val_loss: 0.5131 - val_accuracy: 0.7573 - val_auc: 0.8265\n",
      "Epoch 228/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5247 - accuracy: 0.7310 - auc: 0.8133 - val_loss: 0.5132 - val_accuracy: 0.7632 - val_auc: 0.8264\n",
      "Epoch 229/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5310 - accuracy: 0.7255 - auc: 0.8081 - val_loss: 0.5152 - val_accuracy: 0.7573 - val_auc: 0.8258\n",
      "Epoch 230/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5265 - accuracy: 0.7237 - auc: 0.8105 - val_loss: 0.5141 - val_accuracy: 0.7573 - val_auc: 0.8265\n",
      "Epoch 231/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5392 - accuracy: 0.7118 - auc: 0.8001 - val_loss: 0.5137 - val_accuracy: 0.7573 - val_auc: 0.8266\n",
      "Epoch 232/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5297 - accuracy: 0.7191 - auc: 0.8100 - val_loss: 0.5136 - val_accuracy: 0.7573 - val_auc: 0.8264\n",
      "Epoch 233/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5393 - accuracy: 0.7109 - auc: 0.7986 - val_loss: 0.5160 - val_accuracy: 0.7544 - val_auc: 0.8249\n",
      "Epoch 234/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5289 - accuracy: 0.7219 - auc: 0.8087 - val_loss: 0.5172 - val_accuracy: 0.7573 - val_auc: 0.8240\n",
      "Epoch 235/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5412 - accuracy: 0.7200 - auc: 0.8008 - val_loss: 0.5158 - val_accuracy: 0.7544 - val_auc: 0.8248\n",
      "Epoch 236/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5369 - accuracy: 0.7301 - auc: 0.8025 - val_loss: 0.5142 - val_accuracy: 0.7602 - val_auc: 0.8259\n",
      "Epoch 237/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5453 - accuracy: 0.7145 - auc: 0.7956 - val_loss: 0.5141 - val_accuracy: 0.7573 - val_auc: 0.8259\n",
      "Epoch 238/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5347 - accuracy: 0.7200 - auc: 0.8046 - val_loss: 0.5170 - val_accuracy: 0.7573 - val_auc: 0.8242\n",
      "Epoch 239/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5269 - accuracy: 0.7246 - auc: 0.8110 - val_loss: 0.5167 - val_accuracy: 0.7632 - val_auc: 0.8242\n",
      "Epoch 240/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5362 - accuracy: 0.7310 - auc: 0.8027 - val_loss: 0.5148 - val_accuracy: 0.7515 - val_auc: 0.8259\n",
      "Epoch 241/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5436 - accuracy: 0.7145 - auc: 0.7957 - val_loss: 0.5147 - val_accuracy: 0.7544 - val_auc: 0.8259\n",
      "Epoch 242/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5237 - accuracy: 0.7356 - auc: 0.8154 - val_loss: 0.5150 - val_accuracy: 0.7544 - val_auc: 0.8252\n",
      "Epoch 243/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5334 - accuracy: 0.7246 - auc: 0.8042 - val_loss: 0.5144 - val_accuracy: 0.7544 - val_auc: 0.8256\n",
      "Epoch 244/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5252 - accuracy: 0.7347 - auc: 0.8134 - val_loss: 0.5124 - val_accuracy: 0.7573 - val_auc: 0.8267\n",
      "Epoch 245/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5365 - accuracy: 0.7283 - auc: 0.8036 - val_loss: 0.5118 - val_accuracy: 0.7544 - val_auc: 0.8273\n",
      "Epoch 246/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5389 - accuracy: 0.7118 - auc: 0.7997 - val_loss: 0.5136 - val_accuracy: 0.7544 - val_auc: 0.8255\n",
      "Epoch 247/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5474 - accuracy: 0.7091 - auc: 0.7940 - val_loss: 0.5130 - val_accuracy: 0.7544 - val_auc: 0.8263\n",
      "Epoch 248/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5413 - accuracy: 0.7164 - auc: 0.7990 - val_loss: 0.5127 - val_accuracy: 0.7515 - val_auc: 0.8266\n",
      "Epoch 249/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5332 - accuracy: 0.7274 - auc: 0.8064 - val_loss: 0.5131 - val_accuracy: 0.7544 - val_auc: 0.8256\n",
      "Epoch 250/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5314 - accuracy: 0.7283 - auc: 0.8074 - val_loss: 0.5121 - val_accuracy: 0.7544 - val_auc: 0.8275\n",
      "Epoch 251/700\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.5343 - accuracy: 0.7228 - auc: 0.8059 - val_loss: 0.5102 - val_accuracy: 0.7602 - val_auc: 0.8275\n",
      "Epoch 252/700\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.5317 - accuracy: 0.7164 - auc: 0.8054 - val_loss: 0.5104 - val_accuracy: 0.7573 - val_auc: 0.8282\n",
      "Epoch 253/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5396 - accuracy: 0.7145 - auc: 0.8010 - val_loss: 0.5110 - val_accuracy: 0.7544 - val_auc: 0.8274\n",
      "Epoch 254/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5279 - accuracy: 0.7237 - auc: 0.8109 - val_loss: 0.5114 - val_accuracy: 0.7573 - val_auc: 0.8276\n",
      "Epoch 255/700\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.5250 - accuracy: 0.7429 - auc: 0.8140 - val_loss: 0.5104 - val_accuracy: 0.7573 - val_auc: 0.8284\n",
      "Epoch 256/700\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.5398 - accuracy: 0.7173 - auc: 0.7992 - val_loss: 0.5097 - val_accuracy: 0.7602 - val_auc: 0.8287\n",
      "Epoch 257/700\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.5375 - accuracy: 0.7155 - auc: 0.8039 - val_loss: 0.5099 - val_accuracy: 0.7661 - val_auc: 0.8288\n",
      "Epoch 258/700\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.5271 - accuracy: 0.7255 - auc: 0.8109 - val_loss: 0.5109 - val_accuracy: 0.7661 - val_auc: 0.8284\n",
      "Epoch 259/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5319 - accuracy: 0.7173 - auc: 0.8047 - val_loss: 0.5109 - val_accuracy: 0.7602 - val_auc: 0.8289\n",
      "Epoch 260/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5305 - accuracy: 0.7200 - auc: 0.8080 - val_loss: 0.5117 - val_accuracy: 0.7602 - val_auc: 0.8278\n",
      "Epoch 261/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5360 - accuracy: 0.7228 - auc: 0.8043 - val_loss: 0.5112 - val_accuracy: 0.7573 - val_auc: 0.8280\n",
      "Epoch 262/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5280 - accuracy: 0.7356 - auc: 0.8115 - val_loss: 0.5123 - val_accuracy: 0.7602 - val_auc: 0.8269\n",
      "Epoch 263/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5276 - accuracy: 0.7438 - auc: 0.8129 - val_loss: 0.5111 - val_accuracy: 0.7632 - val_auc: 0.8276\n",
      "Epoch 264/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5324 - accuracy: 0.7091 - auc: 0.8075 - val_loss: 0.5119 - val_accuracy: 0.7632 - val_auc: 0.8273\n",
      "Epoch 265/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5344 - accuracy: 0.7274 - auc: 0.8066 - val_loss: 0.5127 - val_accuracy: 0.7632 - val_auc: 0.8268\n",
      "Epoch 266/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5334 - accuracy: 0.7164 - auc: 0.8036 - val_loss: 0.5135 - val_accuracy: 0.7632 - val_auc: 0.8262\n",
      "Epoch 267/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5297 - accuracy: 0.7374 - auc: 0.8092 - val_loss: 0.5126 - val_accuracy: 0.7632 - val_auc: 0.8266\n",
      "Epoch 268/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5262 - accuracy: 0.7246 - auc: 0.8119 - val_loss: 0.5114 - val_accuracy: 0.7602 - val_auc: 0.8275\n",
      "Epoch 269/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5270 - accuracy: 0.7255 - auc: 0.8105 - val_loss: 0.5110 - val_accuracy: 0.7602 - val_auc: 0.8280\n",
      "Epoch 270/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5241 - accuracy: 0.7301 - auc: 0.8145 - val_loss: 0.5105 - val_accuracy: 0.7602 - val_auc: 0.8285\n",
      "Epoch 271/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5307 - accuracy: 0.7136 - auc: 0.8061 - val_loss: 0.5101 - val_accuracy: 0.7573 - val_auc: 0.8284\n",
      "Epoch 272/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5272 - accuracy: 0.7356 - auc: 0.8145 - val_loss: 0.5110 - val_accuracy: 0.7602 - val_auc: 0.8279\n",
      "Epoch 273/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5259 - accuracy: 0.7237 - auc: 0.8144 - val_loss: 0.5112 - val_accuracy: 0.7602 - val_auc: 0.8279\n",
      "Epoch 274/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5297 - accuracy: 0.7228 - auc: 0.8087 - val_loss: 0.5129 - val_accuracy: 0.7573 - val_auc: 0.8268\n",
      "Epoch 275/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5250 - accuracy: 0.7319 - auc: 0.8135 - val_loss: 0.5115 - val_accuracy: 0.7544 - val_auc: 0.8278\n",
      "Epoch 276/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5257 - accuracy: 0.7365 - auc: 0.8131 - val_loss: 0.5133 - val_accuracy: 0.7573 - val_auc: 0.8267\n",
      "Epoch 277/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5278 - accuracy: 0.7219 - auc: 0.8105 - val_loss: 0.5099 - val_accuracy: 0.7632 - val_auc: 0.8293\n",
      "Epoch 278/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5252 - accuracy: 0.7246 - auc: 0.8128 - val_loss: 0.5107 - val_accuracy: 0.7632 - val_auc: 0.8288\n",
      "Epoch 279/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5306 - accuracy: 0.7292 - auc: 0.8088 - val_loss: 0.5106 - val_accuracy: 0.7661 - val_auc: 0.8290\n",
      "Epoch 280/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5396 - accuracy: 0.7136 - auc: 0.7999 - val_loss: 0.5113 - val_accuracy: 0.7661 - val_auc: 0.8286\n",
      "Epoch 281/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5391 - accuracy: 0.6990 - auc: 0.7983 - val_loss: 0.5105 - val_accuracy: 0.7719 - val_auc: 0.8290\n",
      "Epoch 282/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5281 - accuracy: 0.7274 - auc: 0.8101 - val_loss: 0.5106 - val_accuracy: 0.7661 - val_auc: 0.8290\n",
      "Epoch 283/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5252 - accuracy: 0.7264 - auc: 0.8131 - val_loss: 0.5099 - val_accuracy: 0.7690 - val_auc: 0.8297\n",
      "Epoch 284/700\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5248 - accuracy: 0.7210 - auc: 0.8117 - val_loss: 0.5095 - val_accuracy: 0.7632 - val_auc: 0.8302\n",
      "Epoch 285/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5191 - accuracy: 0.7356 - auc: 0.8171 - val_loss: 0.5091 - val_accuracy: 0.7661 - val_auc: 0.8303\n",
      "Epoch 286/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5308 - accuracy: 0.7255 - auc: 0.8080 - val_loss: 0.5082 - val_accuracy: 0.7632 - val_auc: 0.8310\n",
      "Epoch 287/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5243 - accuracy: 0.7457 - auc: 0.8139 - val_loss: 0.5077 - val_accuracy: 0.7632 - val_auc: 0.8313\n",
      "Epoch 288/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5268 - accuracy: 0.7182 - auc: 0.8111 - val_loss: 0.5087 - val_accuracy: 0.7632 - val_auc: 0.8307\n",
      "Epoch 289/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5164 - accuracy: 0.7292 - auc: 0.8217 - val_loss: 0.5081 - val_accuracy: 0.7632 - val_auc: 0.8311\n",
      "Epoch 290/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5205 - accuracy: 0.7310 - auc: 0.8159 - val_loss: 0.5088 - val_accuracy: 0.7661 - val_auc: 0.8306\n",
      "Epoch 291/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5238 - accuracy: 0.7347 - auc: 0.8153 - val_loss: 0.5092 - val_accuracy: 0.7661 - val_auc: 0.8307\n",
      "Epoch 292/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5249 - accuracy: 0.7200 - auc: 0.8125 - val_loss: 0.5079 - val_accuracy: 0.7661 - val_auc: 0.8313\n",
      "Epoch 293/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5352 - accuracy: 0.7191 - auc: 0.8063 - val_loss: 0.5086 - val_accuracy: 0.7661 - val_auc: 0.8310\n",
      "Epoch 294/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5312 - accuracy: 0.7328 - auc: 0.8097 - val_loss: 0.5093 - val_accuracy: 0.7719 - val_auc: 0.8306\n",
      "Epoch 295/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5272 - accuracy: 0.7274 - auc: 0.8137 - val_loss: 0.5118 - val_accuracy: 0.7602 - val_auc: 0.8286\n",
      "Epoch 296/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5291 - accuracy: 0.7292 - auc: 0.8085 - val_loss: 0.5107 - val_accuracy: 0.7602 - val_auc: 0.8291\n",
      "Epoch 297/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5313 - accuracy: 0.7246 - auc: 0.8069 - val_loss: 0.5099 - val_accuracy: 0.7690 - val_auc: 0.8299\n",
      "Epoch 298/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5208 - accuracy: 0.7219 - auc: 0.8148 - val_loss: 0.5092 - val_accuracy: 0.7690 - val_auc: 0.8306\n",
      "Epoch 299/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5222 - accuracy: 0.7338 - auc: 0.8162 - val_loss: 0.5096 - val_accuracy: 0.7690 - val_auc: 0.8300\n",
      "Epoch 300/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5290 - accuracy: 0.7310 - auc: 0.8104 - val_loss: 0.5091 - val_accuracy: 0.7690 - val_auc: 0.8301\n",
      "Epoch 301/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5242 - accuracy: 0.7237 - auc: 0.8151 - val_loss: 0.5095 - val_accuracy: 0.7690 - val_auc: 0.8301\n",
      "Epoch 302/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5334 - accuracy: 0.7237 - auc: 0.8056 - val_loss: 0.5094 - val_accuracy: 0.7661 - val_auc: 0.8299\n",
      "Epoch 303/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5340 - accuracy: 0.7219 - auc: 0.8047 - val_loss: 0.5082 - val_accuracy: 0.7632 - val_auc: 0.8309\n",
      "Epoch 304/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5156 - accuracy: 0.7447 - auc: 0.8228 - val_loss: 0.5079 - val_accuracy: 0.7632 - val_auc: 0.8313\n",
      "Epoch 305/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5307 - accuracy: 0.7274 - auc: 0.8084 - val_loss: 0.5088 - val_accuracy: 0.7661 - val_auc: 0.8302\n",
      "Epoch 306/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5228 - accuracy: 0.7301 - auc: 0.8150 - val_loss: 0.5090 - val_accuracy: 0.7690 - val_auc: 0.8304\n",
      "Epoch 307/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5298 - accuracy: 0.7328 - auc: 0.8091 - val_loss: 0.5087 - val_accuracy: 0.7690 - val_auc: 0.8302\n",
      "Epoch 308/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5175 - accuracy: 0.7383 - auc: 0.8193 - val_loss: 0.5087 - val_accuracy: 0.7661 - val_auc: 0.8303\n",
      "Epoch 309/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5248 - accuracy: 0.7310 - auc: 0.8147 - val_loss: 0.5117 - val_accuracy: 0.7602 - val_auc: 0.8281\n",
      "Epoch 310/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5272 - accuracy: 0.7328 - auc: 0.8130 - val_loss: 0.5094 - val_accuracy: 0.7632 - val_auc: 0.8296\n",
      "Epoch 311/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5294 - accuracy: 0.7319 - auc: 0.8118 - val_loss: 0.5080 - val_accuracy: 0.7632 - val_auc: 0.8310\n",
      "Epoch 312/700\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5265 - accuracy: 0.7301 - auc: 0.8131 - val_loss: 0.5075 - val_accuracy: 0.7690 - val_auc: 0.8312\n",
      "Epoch 313/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5215 - accuracy: 0.7356 - auc: 0.8162 - val_loss: 0.5069 - val_accuracy: 0.7690 - val_auc: 0.8318\n",
      "Epoch 314/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5300 - accuracy: 0.7200 - auc: 0.8070 - val_loss: 0.5058 - val_accuracy: 0.7661 - val_auc: 0.8323\n",
      "Epoch 315/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5184 - accuracy: 0.7374 - auc: 0.8205 - val_loss: 0.5059 - val_accuracy: 0.7661 - val_auc: 0.8321\n",
      "Epoch 316/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5206 - accuracy: 0.7283 - auc: 0.8173 - val_loss: 0.5058 - val_accuracy: 0.7661 - val_auc: 0.8327\n",
      "Epoch 317/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5232 - accuracy: 0.7292 - auc: 0.8153 - val_loss: 0.5058 - val_accuracy: 0.7661 - val_auc: 0.8323\n",
      "Epoch 318/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5336 - accuracy: 0.7155 - auc: 0.8043 - val_loss: 0.5080 - val_accuracy: 0.7719 - val_auc: 0.8307\n",
      "Epoch 319/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5300 - accuracy: 0.7365 - auc: 0.8104 - val_loss: 0.5081 - val_accuracy: 0.7690 - val_auc: 0.8308\n",
      "Epoch 320/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5164 - accuracy: 0.7319 - auc: 0.8211 - val_loss: 0.5097 - val_accuracy: 0.7602 - val_auc: 0.8300\n",
      "Epoch 321/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5237 - accuracy: 0.7292 - auc: 0.8153 - val_loss: 0.5119 - val_accuracy: 0.7661 - val_auc: 0.8282\n",
      "Epoch 322/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5414 - accuracy: 0.7200 - auc: 0.7998 - val_loss: 0.5108 - val_accuracy: 0.7632 - val_auc: 0.8291\n",
      "Epoch 323/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5228 - accuracy: 0.7210 - auc: 0.8137 - val_loss: 0.5117 - val_accuracy: 0.7661 - val_auc: 0.8286\n",
      "Epoch 324/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5104 - accuracy: 0.7420 - auc: 0.8270 - val_loss: 0.5094 - val_accuracy: 0.7661 - val_auc: 0.8296\n",
      "Epoch 325/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5346 - accuracy: 0.7173 - auc: 0.8061 - val_loss: 0.5079 - val_accuracy: 0.7661 - val_auc: 0.8305\n",
      "Epoch 326/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5158 - accuracy: 0.7338 - auc: 0.8226 - val_loss: 0.5079 - val_accuracy: 0.7632 - val_auc: 0.8303\n",
      "Epoch 327/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5165 - accuracy: 0.7274 - auc: 0.8203 - val_loss: 0.5090 - val_accuracy: 0.7661 - val_auc: 0.8300\n",
      "Epoch 328/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5332 - accuracy: 0.7301 - auc: 0.8062 - val_loss: 0.5094 - val_accuracy: 0.7661 - val_auc: 0.8299\n",
      "Epoch 329/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5252 - accuracy: 0.7292 - auc: 0.8135 - val_loss: 0.5096 - val_accuracy: 0.7661 - val_auc: 0.8298\n",
      "Epoch 330/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5311 - accuracy: 0.7182 - auc: 0.8067 - val_loss: 0.5095 - val_accuracy: 0.7602 - val_auc: 0.8299\n",
      "Epoch 331/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5288 - accuracy: 0.7210 - auc: 0.8091 - val_loss: 0.5093 - val_accuracy: 0.7602 - val_auc: 0.8301\n",
      "Epoch 332/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5283 - accuracy: 0.7274 - auc: 0.8098 - val_loss: 0.5079 - val_accuracy: 0.7661 - val_auc: 0.8313\n",
      "Epoch 333/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5275 - accuracy: 0.7438 - auc: 0.8126 - val_loss: 0.5078 - val_accuracy: 0.7632 - val_auc: 0.8316\n",
      "Epoch 334/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5250 - accuracy: 0.7264 - auc: 0.8125 - val_loss: 0.5058 - val_accuracy: 0.7690 - val_auc: 0.8326\n",
      "Epoch 335/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5320 - accuracy: 0.7292 - auc: 0.8074 - val_loss: 0.5075 - val_accuracy: 0.7690 - val_auc: 0.8319\n",
      "Epoch 336/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5284 - accuracy: 0.7292 - auc: 0.8097 - val_loss: 0.5079 - val_accuracy: 0.7632 - val_auc: 0.8314\n",
      "Epoch 337/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5249 - accuracy: 0.7383 - auc: 0.8152 - val_loss: 0.5069 - val_accuracy: 0.7661 - val_auc: 0.8326\n",
      "Epoch 338/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5214 - accuracy: 0.7274 - auc: 0.8172 - val_loss: 0.5069 - val_accuracy: 0.7690 - val_auc: 0.8320\n",
      "Epoch 339/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5273 - accuracy: 0.7338 - auc: 0.8110 - val_loss: 0.5056 - val_accuracy: 0.7661 - val_auc: 0.8326\n",
      "Epoch 340/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5288 - accuracy: 0.7274 - auc: 0.8103 - val_loss: 0.5071 - val_accuracy: 0.7719 - val_auc: 0.8322\n",
      "Epoch 341/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5125 - accuracy: 0.7438 - auc: 0.8236 - val_loss: 0.5075 - val_accuracy: 0.7749 - val_auc: 0.8319\n",
      "Epoch 342/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5196 - accuracy: 0.7429 - auc: 0.8194 - val_loss: 0.5060 - val_accuracy: 0.7661 - val_auc: 0.8328\n",
      "Epoch 343/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5192 - accuracy: 0.7347 - auc: 0.8184 - val_loss: 0.5067 - val_accuracy: 0.7690 - val_auc: 0.8322\n",
      "Epoch 344/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5238 - accuracy: 0.7301 - auc: 0.8139 - val_loss: 0.5054 - val_accuracy: 0.7749 - val_auc: 0.8330\n",
      "Epoch 345/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5324 - accuracy: 0.7155 - auc: 0.8074 - val_loss: 0.5063 - val_accuracy: 0.7690 - val_auc: 0.8325\n",
      "Epoch 346/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5244 - accuracy: 0.7292 - auc: 0.8129 - val_loss: 0.5059 - val_accuracy: 0.7749 - val_auc: 0.8336\n",
      "Epoch 347/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5197 - accuracy: 0.7356 - auc: 0.8188 - val_loss: 0.5064 - val_accuracy: 0.7690 - val_auc: 0.8324\n",
      "Epoch 348/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5226 - accuracy: 0.7365 - auc: 0.8153 - val_loss: 0.5075 - val_accuracy: 0.7690 - val_auc: 0.8321\n",
      "Epoch 349/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5269 - accuracy: 0.7164 - auc: 0.8094 - val_loss: 0.5064 - val_accuracy: 0.7690 - val_auc: 0.8324\n",
      "Epoch 350/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5196 - accuracy: 0.7484 - auc: 0.8183 - val_loss: 0.5056 - val_accuracy: 0.7749 - val_auc: 0.8335\n",
      "Epoch 351/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5274 - accuracy: 0.7338 - auc: 0.8123 - val_loss: 0.5074 - val_accuracy: 0.7632 - val_auc: 0.8322\n",
      "Epoch 352/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5185 - accuracy: 0.7228 - auc: 0.8181 - val_loss: 0.5056 - val_accuracy: 0.7690 - val_auc: 0.8335\n",
      "Epoch 353/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5168 - accuracy: 0.7301 - auc: 0.8184 - val_loss: 0.5046 - val_accuracy: 0.7778 - val_auc: 0.8346\n",
      "Epoch 354/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5244 - accuracy: 0.7237 - auc: 0.8140 - val_loss: 0.5064 - val_accuracy: 0.7778 - val_auc: 0.8334\n",
      "Epoch 355/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5228 - accuracy: 0.7301 - auc: 0.8161 - val_loss: 0.5039 - val_accuracy: 0.7778 - val_auc: 0.8342\n",
      "Epoch 356/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5163 - accuracy: 0.7374 - auc: 0.8210 - val_loss: 0.5058 - val_accuracy: 0.7778 - val_auc: 0.8340\n",
      "Epoch 357/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5161 - accuracy: 0.7356 - auc: 0.8222 - val_loss: 0.5043 - val_accuracy: 0.7749 - val_auc: 0.8344\n",
      "Epoch 358/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5208 - accuracy: 0.7328 - auc: 0.8176 - val_loss: 0.5053 - val_accuracy: 0.7719 - val_auc: 0.8339\n",
      "Epoch 359/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5144 - accuracy: 0.7283 - auc: 0.8223 - val_loss: 0.5061 - val_accuracy: 0.7807 - val_auc: 0.8337\n",
      "Epoch 360/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5200 - accuracy: 0.7392 - auc: 0.8206 - val_loss: 0.5063 - val_accuracy: 0.7836 - val_auc: 0.8333\n",
      "Epoch 361/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5197 - accuracy: 0.7328 - auc: 0.8211 - val_loss: 0.5052 - val_accuracy: 0.7749 - val_auc: 0.8338\n",
      "Epoch 362/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5281 - accuracy: 0.7182 - auc: 0.8115 - val_loss: 0.5044 - val_accuracy: 0.7778 - val_auc: 0.8340\n",
      "Epoch 363/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5221 - accuracy: 0.7274 - auc: 0.8138 - val_loss: 0.5040 - val_accuracy: 0.7749 - val_auc: 0.8344\n",
      "Epoch 364/700\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.5219 - accuracy: 0.7301 - auc: 0.8150 - val_loss: 0.5037 - val_accuracy: 0.7690 - val_auc: 0.8342\n",
      "Epoch 365/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5215 - accuracy: 0.7264 - auc: 0.8172 - val_loss: 0.5054 - val_accuracy: 0.7719 - val_auc: 0.8333\n",
      "Epoch 366/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5241 - accuracy: 0.7402 - auc: 0.8144 - val_loss: 0.5045 - val_accuracy: 0.7719 - val_auc: 0.8338\n",
      "Epoch 367/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5224 - accuracy: 0.7383 - auc: 0.8173 - val_loss: 0.5061 - val_accuracy: 0.7749 - val_auc: 0.8331\n",
      "Epoch 368/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5257 - accuracy: 0.7283 - auc: 0.8116 - val_loss: 0.5057 - val_accuracy: 0.7719 - val_auc: 0.8330\n",
      "Epoch 369/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5155 - accuracy: 0.7356 - auc: 0.8220 - val_loss: 0.5045 - val_accuracy: 0.7719 - val_auc: 0.8341\n",
      "Epoch 370/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5264 - accuracy: 0.7328 - auc: 0.8109 - val_loss: 0.5046 - val_accuracy: 0.7749 - val_auc: 0.8344\n",
      "Epoch 371/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5176 - accuracy: 0.7383 - auc: 0.8210 - val_loss: 0.5047 - val_accuracy: 0.7749 - val_auc: 0.8340\n",
      "Epoch 372/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5299 - accuracy: 0.7264 - auc: 0.8103 - val_loss: 0.5042 - val_accuracy: 0.7719 - val_auc: 0.8345\n",
      "Epoch 373/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5270 - accuracy: 0.7283 - auc: 0.8129 - val_loss: 0.5052 - val_accuracy: 0.7749 - val_auc: 0.8342\n",
      "Epoch 374/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5166 - accuracy: 0.7383 - auc: 0.8235 - val_loss: 0.5044 - val_accuracy: 0.7749 - val_auc: 0.8345\n",
      "Epoch 375/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5181 - accuracy: 0.7237 - auc: 0.8193 - val_loss: 0.5047 - val_accuracy: 0.7778 - val_auc: 0.8342\n",
      "Epoch 376/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5231 - accuracy: 0.7328 - auc: 0.8146 - val_loss: 0.5045 - val_accuracy: 0.7749 - val_auc: 0.8343\n",
      "Epoch 377/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5137 - accuracy: 0.7447 - auc: 0.8229 - val_loss: 0.5041 - val_accuracy: 0.7719 - val_auc: 0.8347\n",
      "Epoch 378/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5216 - accuracy: 0.7237 - auc: 0.8159 - val_loss: 0.5043 - val_accuracy: 0.7719 - val_auc: 0.8344\n",
      "Epoch 379/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5173 - accuracy: 0.7274 - auc: 0.8204 - val_loss: 0.5042 - val_accuracy: 0.7719 - val_auc: 0.8342\n",
      "Epoch 380/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5311 - accuracy: 0.7328 - auc: 0.8106 - val_loss: 0.5033 - val_accuracy: 0.7749 - val_auc: 0.8350\n",
      "Epoch 381/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5164 - accuracy: 0.7457 - auc: 0.8220 - val_loss: 0.5030 - val_accuracy: 0.7749 - val_auc: 0.8350\n",
      "Epoch 382/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5193 - accuracy: 0.7319 - auc: 0.8180 - val_loss: 0.5044 - val_accuracy: 0.7778 - val_auc: 0.8340\n",
      "Epoch 383/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5247 - accuracy: 0.7457 - auc: 0.8135 - val_loss: 0.5049 - val_accuracy: 0.7778 - val_auc: 0.8342\n",
      "Epoch 384/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5187 - accuracy: 0.7264 - auc: 0.8180 - val_loss: 0.5035 - val_accuracy: 0.7807 - val_auc: 0.8348\n",
      "Epoch 385/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5154 - accuracy: 0.7502 - auc: 0.8211 - val_loss: 0.5037 - val_accuracy: 0.7749 - val_auc: 0.8345\n",
      "Epoch 386/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5150 - accuracy: 0.7338 - auc: 0.8220 - val_loss: 0.5029 - val_accuracy: 0.7749 - val_auc: 0.8348\n",
      "Epoch 387/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5256 - accuracy: 0.7182 - auc: 0.8127 - val_loss: 0.5046 - val_accuracy: 0.7749 - val_auc: 0.8342\n",
      "Epoch 388/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5189 - accuracy: 0.7274 - auc: 0.8175 - val_loss: 0.5029 - val_accuracy: 0.7749 - val_auc: 0.8347\n",
      "Epoch 389/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5187 - accuracy: 0.7338 - auc: 0.8205 - val_loss: 0.5040 - val_accuracy: 0.7749 - val_auc: 0.8338\n",
      "Epoch 390/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5106 - accuracy: 0.7328 - auc: 0.8258 - val_loss: 0.5041 - val_accuracy: 0.7719 - val_auc: 0.8338\n",
      "Epoch 391/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5105 - accuracy: 0.7466 - auc: 0.8261 - val_loss: 0.5047 - val_accuracy: 0.7690 - val_auc: 0.8335\n",
      "Epoch 392/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5171 - accuracy: 0.7347 - auc: 0.8209 - val_loss: 0.5049 - val_accuracy: 0.7749 - val_auc: 0.8338\n",
      "Epoch 393/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5237 - accuracy: 0.7274 - auc: 0.8144 - val_loss: 0.5037 - val_accuracy: 0.7719 - val_auc: 0.8344\n",
      "Epoch 394/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5268 - accuracy: 0.7374 - auc: 0.8119 - val_loss: 0.5038 - val_accuracy: 0.7749 - val_auc: 0.8343\n",
      "Epoch 395/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5169 - accuracy: 0.7392 - auc: 0.8208 - val_loss: 0.5028 - val_accuracy: 0.7749 - val_auc: 0.8352\n",
      "Epoch 396/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5240 - accuracy: 0.7246 - auc: 0.8131 - val_loss: 0.5040 - val_accuracy: 0.7719 - val_auc: 0.8346\n",
      "Epoch 397/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5263 - accuracy: 0.7429 - auc: 0.8127 - val_loss: 0.5037 - val_accuracy: 0.7749 - val_auc: 0.8347\n",
      "Epoch 398/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5112 - accuracy: 0.7264 - auc: 0.8257 - val_loss: 0.5036 - val_accuracy: 0.7719 - val_auc: 0.8347\n",
      "Epoch 399/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5085 - accuracy: 0.7383 - auc: 0.8270 - val_loss: 0.5028 - val_accuracy: 0.7749 - val_auc: 0.8354\n",
      "Epoch 400/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5139 - accuracy: 0.7457 - auc: 0.8258 - val_loss: 0.5045 - val_accuracy: 0.7719 - val_auc: 0.8342\n",
      "Epoch 401/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5205 - accuracy: 0.7255 - auc: 0.8165 - val_loss: 0.5058 - val_accuracy: 0.7749 - val_auc: 0.8335\n",
      "Epoch 402/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5240 - accuracy: 0.7420 - auc: 0.8149 - val_loss: 0.5044 - val_accuracy: 0.7778 - val_auc: 0.8343\n",
      "Epoch 403/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5158 - accuracy: 0.7310 - auc: 0.8224 - val_loss: 0.5042 - val_accuracy: 0.7749 - val_auc: 0.8343\n",
      "Epoch 404/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5328 - accuracy: 0.7246 - auc: 0.8082 - val_loss: 0.5061 - val_accuracy: 0.7749 - val_auc: 0.8332\n",
      "Epoch 405/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5267 - accuracy: 0.7127 - auc: 0.8107 - val_loss: 0.5038 - val_accuracy: 0.7749 - val_auc: 0.8349\n",
      "Epoch 406/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5243 - accuracy: 0.7356 - auc: 0.8144 - val_loss: 0.5049 - val_accuracy: 0.7719 - val_auc: 0.8339\n",
      "Epoch 407/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5229 - accuracy: 0.7383 - auc: 0.8156 - val_loss: 0.5047 - val_accuracy: 0.7719 - val_auc: 0.8343\n",
      "Epoch 408/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5245 - accuracy: 0.7319 - auc: 0.8130 - val_loss: 0.5049 - val_accuracy: 0.7749 - val_auc: 0.8340\n",
      "Epoch 409/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5217 - accuracy: 0.7383 - auc: 0.8165 - val_loss: 0.5038 - val_accuracy: 0.7778 - val_auc: 0.8347\n",
      "Epoch 410/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5048 - accuracy: 0.7420 - auc: 0.8296 - val_loss: 0.5022 - val_accuracy: 0.7836 - val_auc: 0.8363\n",
      "Epoch 411/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5199 - accuracy: 0.7255 - auc: 0.8178 - val_loss: 0.5032 - val_accuracy: 0.7807 - val_auc: 0.8355\n",
      "Epoch 412/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5196 - accuracy: 0.7191 - auc: 0.8175 - val_loss: 0.5034 - val_accuracy: 0.7807 - val_auc: 0.8354\n",
      "Epoch 413/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5109 - accuracy: 0.7411 - auc: 0.8261 - val_loss: 0.5027 - val_accuracy: 0.7719 - val_auc: 0.8353\n",
      "Epoch 414/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5123 - accuracy: 0.7383 - auc: 0.8216 - val_loss: 0.5046 - val_accuracy: 0.7690 - val_auc: 0.8340\n",
      "Epoch 415/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5182 - accuracy: 0.7383 - auc: 0.8214 - val_loss: 0.5031 - val_accuracy: 0.7719 - val_auc: 0.8352\n",
      "Epoch 416/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5113 - accuracy: 0.7466 - auc: 0.8257 - val_loss: 0.5002 - val_accuracy: 0.7749 - val_auc: 0.8365\n",
      "Epoch 417/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5147 - accuracy: 0.7319 - auc: 0.8216 - val_loss: 0.5013 - val_accuracy: 0.7778 - val_auc: 0.8364\n",
      "Epoch 418/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5149 - accuracy: 0.7347 - auc: 0.8216 - val_loss: 0.5016 - val_accuracy: 0.7749 - val_auc: 0.8360\n",
      "Epoch 419/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5125 - accuracy: 0.7420 - auc: 0.8258 - val_loss: 0.5020 - val_accuracy: 0.7749 - val_auc: 0.8360\n",
      "Epoch 420/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5136 - accuracy: 0.7374 - auc: 0.8236 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8364\n",
      "Epoch 421/700\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5112 - accuracy: 0.7383 - auc: 0.8250 - val_loss: 0.4991 - val_accuracy: 0.7778 - val_auc: 0.8372\n",
      "Epoch 422/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5143 - accuracy: 0.7402 - auc: 0.8231 - val_loss: 0.4987 - val_accuracy: 0.7749 - val_auc: 0.8371\n",
      "Epoch 423/700\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5117 - accuracy: 0.7328 - auc: 0.8241 - val_loss: 0.4985 - val_accuracy: 0.7749 - val_auc: 0.8373\n",
      "Epoch 424/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5084 - accuracy: 0.7429 - auc: 0.8288 - val_loss: 0.4989 - val_accuracy: 0.7749 - val_auc: 0.8373\n",
      "Epoch 425/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5073 - accuracy: 0.7383 - auc: 0.8278 - val_loss: 0.4988 - val_accuracy: 0.7749 - val_auc: 0.8373\n",
      "Epoch 426/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5110 - accuracy: 0.7365 - auc: 0.8248 - val_loss: 0.4995 - val_accuracy: 0.7719 - val_auc: 0.8369\n",
      "Epoch 427/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5126 - accuracy: 0.7466 - auc: 0.8252 - val_loss: 0.5003 - val_accuracy: 0.7778 - val_auc: 0.8368\n",
      "Epoch 428/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5042 - accuracy: 0.7383 - auc: 0.8301 - val_loss: 0.4998 - val_accuracy: 0.7749 - val_auc: 0.8369\n",
      "Epoch 429/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5085 - accuracy: 0.7402 - auc: 0.8261 - val_loss: 0.5004 - val_accuracy: 0.7749 - val_auc: 0.8365\n",
      "Epoch 430/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5105 - accuracy: 0.7374 - auc: 0.8245 - val_loss: 0.5013 - val_accuracy: 0.7749 - val_auc: 0.8367\n",
      "Epoch 431/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5155 - accuracy: 0.7301 - auc: 0.8191 - val_loss: 0.5009 - val_accuracy: 0.7836 - val_auc: 0.8368\n",
      "Epoch 432/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5120 - accuracy: 0.7392 - auc: 0.8250 - val_loss: 0.4995 - val_accuracy: 0.7749 - val_auc: 0.8372\n",
      "Epoch 433/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5087 - accuracy: 0.7383 - auc: 0.8269 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8368\n",
      "Epoch 434/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5253 - accuracy: 0.7383 - auc: 0.8117 - val_loss: 0.5000 - val_accuracy: 0.7749 - val_auc: 0.8364\n",
      "Epoch 435/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5112 - accuracy: 0.7283 - auc: 0.8245 - val_loss: 0.4999 - val_accuracy: 0.7719 - val_auc: 0.8366\n",
      "Epoch 436/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5133 - accuracy: 0.7246 - auc: 0.8219 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8364\n",
      "Epoch 437/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5189 - accuracy: 0.7319 - auc: 0.8176 - val_loss: 0.5002 - val_accuracy: 0.7836 - val_auc: 0.8367\n",
      "Epoch 438/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5171 - accuracy: 0.7328 - auc: 0.8203 - val_loss: 0.5005 - val_accuracy: 0.7807 - val_auc: 0.8368\n",
      "Epoch 439/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5118 - accuracy: 0.7301 - auc: 0.8257 - val_loss: 0.4992 - val_accuracy: 0.7807 - val_auc: 0.8373\n",
      "Epoch 440/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5191 - accuracy: 0.7374 - auc: 0.8190 - val_loss: 0.4999 - val_accuracy: 0.7749 - val_auc: 0.8372\n",
      "Epoch 441/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5136 - accuracy: 0.7310 - auc: 0.8229 - val_loss: 0.5001 - val_accuracy: 0.7807 - val_auc: 0.8368\n",
      "Epoch 442/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5124 - accuracy: 0.7466 - auc: 0.8234 - val_loss: 0.5001 - val_accuracy: 0.7749 - val_auc: 0.8369\n",
      "Epoch 443/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5109 - accuracy: 0.7301 - auc: 0.8240 - val_loss: 0.4990 - val_accuracy: 0.7778 - val_auc: 0.8372\n",
      "Epoch 444/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5223 - accuracy: 0.7365 - auc: 0.8140 - val_loss: 0.4987 - val_accuracy: 0.7778 - val_auc: 0.8373\n",
      "Epoch 445/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5202 - accuracy: 0.7402 - auc: 0.8204 - val_loss: 0.4999 - val_accuracy: 0.7807 - val_auc: 0.8368\n",
      "Epoch 446/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5158 - accuracy: 0.7429 - auc: 0.8211 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8367\n",
      "Epoch 447/700\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5149 - accuracy: 0.7365 - auc: 0.8196 - val_loss: 0.4998 - val_accuracy: 0.7778 - val_auc: 0.8364\n",
      "Epoch 448/700\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5204 - accuracy: 0.7292 - auc: 0.8169 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8362\n",
      "Epoch 449/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5131 - accuracy: 0.7338 - auc: 0.8217 - val_loss: 0.5010 - val_accuracy: 0.7778 - val_auc: 0.8363\n",
      "Epoch 450/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5184 - accuracy: 0.7264 - auc: 0.8196 - val_loss: 0.5011 - val_accuracy: 0.7778 - val_auc: 0.8361\n",
      "Epoch 451/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5131 - accuracy: 0.7374 - auc: 0.8232 - val_loss: 0.5001 - val_accuracy: 0.7807 - val_auc: 0.8367\n",
      "Epoch 452/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5071 - accuracy: 0.7502 - auc: 0.8292 - val_loss: 0.5017 - val_accuracy: 0.7749 - val_auc: 0.8358\n",
      "Epoch 453/700\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5054 - accuracy: 0.7347 - auc: 0.8283 - val_loss: 0.5002 - val_accuracy: 0.7778 - val_auc: 0.8368\n",
      "Epoch 00453: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=RMSprop(lr=0.00001, decay=1e-6), loss='categorical_crossentropy', metrics=[METRICS])\n",
    "\n",
    "# Save best model\n",
    "best_model_save = ModelCheckpoint('angry_ravdess.hdf5', save_best_only=True, monitor='val_loss', mode='auto')\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 30,  verbose=1, mode='auto')\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=700, validation_data=(X_test, y_test), callbacks=[early_stopping, best_model_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r80aTujCRt0v"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('(True Negatives): ', cm[0][0])\n",
    "  print('(False Positives): ', cm[0][1])\n",
    "  print('(False Negatives): ', cm[1][0])\n",
    "  print('(True Positives): ', cm[1][1])\n",
    "  print('Total emotions_happy: ', np.sum(cm[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1596107053752,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "UMYnrL7YRw65",
    "outputId": "5e0b95e5-c17e-4771-ec2c-6f951f683e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.4985100030899048\n",
      "accuracy :  0.7748538255691528\n",
      "auc :  0.8373430967330933\n",
      "\n",
      "(True Negatives):  155\n",
      "(False Positives):  46\n",
      "(False Negatives):  31\n",
      "(True Positives):  110\n",
      "Total emotions_happy:  141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       201\n",
      "           1       0.71      0.78      0.74       141\n",
      "\n",
      "    accuracy                           0.77       342\n",
      "   macro avg       0.77      0.78      0.77       342\n",
      "weighted avg       0.78      0.77      0.78       342\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = load_model('angry_ravdess.hdf5')\n",
    "test_predictions_baseline = model.predict(X_test)\n",
    "baseline_results = model.evaluate(X_test, y_test,\n",
    "                                  batch_size= 64, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "y_pred = np.argmax(test_predictions_baseline, axis= 1)\n",
    "yy_test = np.argmax(y_test, axis  = 1)\n",
    "plot_cm(yy_test, y_pred)\n",
    "\n",
    "print(classification_report(yy_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO7WMWQ1Aljl"
   },
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1596107159591,
     "user": {
      "displayName": "BARRY Nene Djenaba",
      "photoUrl": "",
      "userId": "09221402848573081539"
     },
     "user_tz": -120
    },
    "id": "l1iShdfBIy_v",
    "outputId": "b1fdc0ef-6018-4d1b-ee3f-0c13b41e17c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.5076913237571716\n",
      "accuracy :  0.7591241002082825\n",
      "auc :  0.8370118737220764\n",
      "\n",
      "(True Negatives):  131\n",
      "(False Positives):  48\n",
      "(False Negatives):  18\n",
      "(True Positives):  77\n",
      "Total emotions_happy:  95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.73      0.80       179\n",
      "           1       0.62      0.81      0.70        95\n",
      "\n",
      "    accuracy                           0.76       274\n",
      "   macro avg       0.75      0.77      0.75       274\n",
      "weighted avg       0.79      0.76      0.76       274\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "val_predictions_baseline = model.predict(X_val)\n",
    "baseline_results = model.evaluate(X_val, y_val,\n",
    "                                  batch_size= 64, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "y_pred_val = np.argmax(val_predictions_baseline, axis= 1)\n",
    "yy_val = np.argmax(y_val, axis  = 1)\n",
    "plot_cm(yy_val, y_pred_val)\n",
    "\n",
    "print(classification_report(yy_val, y_pred_val))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPOS/NTCQLbKOU6s9tEtPDC",
   "name": "deep_angry_RAVDESS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
